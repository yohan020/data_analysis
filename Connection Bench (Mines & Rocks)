{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8670316,"sourceType":"datasetVersion","datasetId":4831849}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:35.812853Z","iopub.execute_input":"2025-09-15T10:56:35.813655Z","iopub.status.idle":"2025-09-15T10:56:35.817431Z","shell.execute_reply.started":"2025-09-15T10:56:35.813630Z","shell.execute_reply":"2025-09-15T10:56:35.816644Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 데이터 불러오기","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/connectionist-bench-mines-and-rocks/Sonar.csv')\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:35.818553Z","iopub.execute_input":"2025-09-15T10:56:35.818833Z","iopub.status.idle":"2025-09-15T10:56:35.857464Z","shell.execute_reply.started":"2025-09-15T10:56:35.818810Z","shell.execute_reply":"2025-09-15T10:56:35.856838Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   Freq_1  Freq_2  Freq_3  Freq_4  Freq_5  Freq_6  Freq_7  Freq_8  Freq_9  \\\n0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n\n   Freq_10  ...  Freq_52  Freq_53  Freq_54  Freq_55  Freq_56  Freq_57  \\\n0   0.2111  ...   0.0027   0.0065   0.0159   0.0072   0.0167   0.0180   \n1   0.2872  ...   0.0084   0.0089   0.0048   0.0094   0.0191   0.0140   \n2   0.6194  ...   0.0232   0.0166   0.0095   0.0180   0.0244   0.0316   \n3   0.1264  ...   0.0121   0.0036   0.0150   0.0085   0.0073   0.0050   \n4   0.4459  ...   0.0031   0.0054   0.0105   0.0110   0.0015   0.0072   \n\n   Freq_58  Freq_59  Freq_60  Label  \n0   0.0084   0.0090   0.0032      R  \n1   0.0049   0.0052   0.0044      R  \n2   0.0164   0.0095   0.0078      R  \n3   0.0044   0.0040   0.0117      R  \n4   0.0048   0.0107   0.0094      R  \n\n[5 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Freq_1</th>\n      <th>Freq_2</th>\n      <th>Freq_3</th>\n      <th>Freq_4</th>\n      <th>Freq_5</th>\n      <th>Freq_6</th>\n      <th>Freq_7</th>\n      <th>Freq_8</th>\n      <th>Freq_9</th>\n      <th>Freq_10</th>\n      <th>...</th>\n      <th>Freq_52</th>\n      <th>Freq_53</th>\n      <th>Freq_54</th>\n      <th>Freq_55</th>\n      <th>Freq_56</th>\n      <th>Freq_57</th>\n      <th>Freq_58</th>\n      <th>Freq_59</th>\n      <th>Freq_60</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# 데이터 탐색","metadata":{}},{"cell_type":"code","source":"df_train.info()\ndf_train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:35.858062Z","iopub.execute_input":"2025-09-15T10:56:35.858226Z","iopub.status.idle":"2025-09-15T10:56:35.943329Z","shell.execute_reply.started":"2025-09-15T10:56:35.858212Z","shell.execute_reply":"2025-09-15T10:56:35.942681Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 208 entries, 0 to 207\nData columns (total 61 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Freq_1   208 non-null    float64\n 1   Freq_2   208 non-null    float64\n 2   Freq_3   208 non-null    float64\n 3   Freq_4   208 non-null    float64\n 4   Freq_5   208 non-null    float64\n 5   Freq_6   208 non-null    float64\n 6   Freq_7   208 non-null    float64\n 7   Freq_8   208 non-null    float64\n 8   Freq_9   208 non-null    float64\n 9   Freq_10  208 non-null    float64\n 10  Freq_11  208 non-null    float64\n 11  Freq_12  208 non-null    float64\n 12  Freq_13  208 non-null    float64\n 13  Freq_14  208 non-null    float64\n 14  Freq_15  208 non-null    float64\n 15  Freq_16  208 non-null    float64\n 16  Freq_17  208 non-null    float64\n 17  Freq_18  208 non-null    float64\n 18  Freq_19  208 non-null    float64\n 19  Freq_20  208 non-null    float64\n 20  Freq_21  208 non-null    float64\n 21  Freq_22  208 non-null    float64\n 22  Freq_23  208 non-null    float64\n 23  Freq_24  208 non-null    float64\n 24  Freq_25  208 non-null    float64\n 25  Freq_26  208 non-null    float64\n 26  Freq_27  208 non-null    float64\n 27  Freq_28  208 non-null    float64\n 28  Freq_29  208 non-null    float64\n 29  Freq_30  208 non-null    float64\n 30  Freq_31  208 non-null    float64\n 31  Freq_32  208 non-null    float64\n 32  Freq_33  208 non-null    float64\n 33  Freq_34  208 non-null    float64\n 34  Freq_35  208 non-null    float64\n 35  Freq_36  208 non-null    float64\n 36  Freq_37  208 non-null    float64\n 37  Freq_38  208 non-null    float64\n 38  Freq_39  208 non-null    float64\n 39  Freq_40  208 non-null    float64\n 40  Freq_41  208 non-null    float64\n 41  Freq_42  208 non-null    float64\n 42  Freq_43  208 non-null    float64\n 43  Freq_44  208 non-null    float64\n 44  Freq_45  208 non-null    float64\n 45  Freq_46  208 non-null    float64\n 46  Freq_47  208 non-null    float64\n 47  Freq_48  208 non-null    float64\n 48  Freq_49  208 non-null    float64\n 49  Freq_50  208 non-null    float64\n 50  Freq_51  208 non-null    float64\n 51  Freq_52  208 non-null    float64\n 52  Freq_53  208 non-null    float64\n 53  Freq_54  208 non-null    float64\n 54  Freq_55  208 non-null    float64\n 55  Freq_56  208 non-null    float64\n 56  Freq_57  208 non-null    float64\n 57  Freq_58  208 non-null    float64\n 58  Freq_59  208 non-null    float64\n 59  Freq_60  208 non-null    float64\n 60  Label    208 non-null    object \ndtypes: float64(60), object(1)\nmemory usage: 99.3+ KB\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"           Freq_1      Freq_2      Freq_3      Freq_4      Freq_5      Freq_6  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \nstd      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \nmin      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \nmax      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n\n           Freq_7      Freq_8      Freq_9     Freq_10  ...     Freq_51  \\\ncount  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \nmean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \nstd      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \nmin      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \nmax      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n\n          Freq_52     Freq_53     Freq_54     Freq_55     Freq_56     Freq_57  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \nstd      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \nmin      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \nmax      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n\n          Freq_58     Freq_59     Freq_60  \ncount  208.000000  208.000000  208.000000  \nmean     0.007949    0.007941    0.006507  \nstd      0.006470    0.006181    0.005031  \nmin      0.000300    0.000100    0.000600  \n25%      0.003600    0.003675    0.003100  \n50%      0.005800    0.006400    0.005300  \n75%      0.010350    0.010325    0.008525  \nmax      0.044000    0.036400    0.043900  \n\n[8 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Freq_1</th>\n      <th>Freq_2</th>\n      <th>Freq_3</th>\n      <th>Freq_4</th>\n      <th>Freq_5</th>\n      <th>Freq_6</th>\n      <th>Freq_7</th>\n      <th>Freq_8</th>\n      <th>Freq_9</th>\n      <th>Freq_10</th>\n      <th>...</th>\n      <th>Freq_51</th>\n      <th>Freq_52</th>\n      <th>Freq_53</th>\n      <th>Freq_54</th>\n      <th>Freq_55</th>\n      <th>Freq_56</th>\n      <th>Freq_57</th>\n      <th>Freq_58</th>\n      <th>Freq_59</th>\n      <th>Freq_60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>...</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.029164</td>\n      <td>0.038437</td>\n      <td>0.043832</td>\n      <td>0.053892</td>\n      <td>0.075202</td>\n      <td>0.104570</td>\n      <td>0.121747</td>\n      <td>0.134799</td>\n      <td>0.178003</td>\n      <td>0.208259</td>\n      <td>...</td>\n      <td>0.016069</td>\n      <td>0.013420</td>\n      <td>0.010709</td>\n      <td>0.010941</td>\n      <td>0.009290</td>\n      <td>0.008222</td>\n      <td>0.007820</td>\n      <td>0.007949</td>\n      <td>0.007941</td>\n      <td>0.006507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.022991</td>\n      <td>0.032960</td>\n      <td>0.038428</td>\n      <td>0.046528</td>\n      <td>0.055552</td>\n      <td>0.059105</td>\n      <td>0.061788</td>\n      <td>0.085152</td>\n      <td>0.118387</td>\n      <td>0.134416</td>\n      <td>...</td>\n      <td>0.012008</td>\n      <td>0.009634</td>\n      <td>0.007060</td>\n      <td>0.007301</td>\n      <td>0.007088</td>\n      <td>0.005736</td>\n      <td>0.005785</td>\n      <td>0.006470</td>\n      <td>0.006181</td>\n      <td>0.005031</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001500</td>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.010200</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.007500</td>\n      <td>0.011300</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000300</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.013350</td>\n      <td>0.016450</td>\n      <td>0.018950</td>\n      <td>0.024375</td>\n      <td>0.038050</td>\n      <td>0.067025</td>\n      <td>0.080900</td>\n      <td>0.080425</td>\n      <td>0.097025</td>\n      <td>0.111275</td>\n      <td>...</td>\n      <td>0.008425</td>\n      <td>0.007275</td>\n      <td>0.005075</td>\n      <td>0.005375</td>\n      <td>0.004150</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003675</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.022800</td>\n      <td>0.030800</td>\n      <td>0.034300</td>\n      <td>0.044050</td>\n      <td>0.062500</td>\n      <td>0.092150</td>\n      <td>0.106950</td>\n      <td>0.112100</td>\n      <td>0.152250</td>\n      <td>0.182400</td>\n      <td>...</td>\n      <td>0.013900</td>\n      <td>0.011400</td>\n      <td>0.009550</td>\n      <td>0.009300</td>\n      <td>0.007500</td>\n      <td>0.006850</td>\n      <td>0.005950</td>\n      <td>0.005800</td>\n      <td>0.006400</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.035550</td>\n      <td>0.047950</td>\n      <td>0.057950</td>\n      <td>0.064500</td>\n      <td>0.100275</td>\n      <td>0.134125</td>\n      <td>0.154000</td>\n      <td>0.169600</td>\n      <td>0.233425</td>\n      <td>0.268700</td>\n      <td>...</td>\n      <td>0.020825</td>\n      <td>0.016725</td>\n      <td>0.014900</td>\n      <td>0.014500</td>\n      <td>0.012100</td>\n      <td>0.010575</td>\n      <td>0.010425</td>\n      <td>0.010350</td>\n      <td>0.010325</td>\n      <td>0.008525</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.137100</td>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>...</td>\n      <td>0.100400</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 60 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# 결측치 확인","metadata":{}},{"cell_type":"code","source":"for a, x in df_train.isna().sum().items():\n    if x != 0:\n        print(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:35.944590Z","iopub.execute_input":"2025-09-15T10:56:35.944793Z","iopub.status.idle":"2025-09-15T10:56:35.949178Z","shell.execute_reply.started":"2025-09-15T10:56:35.944777Z","shell.execute_reply":"2025-09-15T10:56:35.948469Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# 바위 vs 지뢰 확인","metadata":{}},{"cell_type":"code","source":"plot_df = df_train.Label.value_counts()\nplot_df.plot(kind=\"bar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:35.949946Z","iopub.execute_input":"2025-09-15T10:56:35.950258Z","iopub.status.idle":"2025-09-15T10:56:36.068662Z","shell.execute_reply.started":"2025-09-15T10:56:35.950234Z","shell.execute_reply":"2025-09-15T10:56:36.067952Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='Label'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# 데이터 셋 준비","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_ds_pd, valid_ds_pd = train_test_split(df_train, test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:36.069372Z","iopub.execute_input":"2025-09-15T10:56:36.069668Z","iopub.status.idle":"2025-09-15T10:56:36.074602Z","shell.execute_reply.started":"2025-09-15T10:56:36.069651Z","shell.execute_reply":"2025-09-15T10:56:36.073884Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import tensorflow_decision_forests as tfdf\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label='Label')\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label='Label')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:36.075176Z","iopub.execute_input":"2025-09-15T10:56:36.075404Z","iopub.status.idle":"2025-09-15T10:56:38.423650Z","shell.execute_reply.started":"2025-09-15T10:56:36.075388Z","shell.execute_reply":"2025-09-15T10:56:38.422853Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<p style=\"margin:0px;\">🌲 Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n        Decision Forests</a> using the same algorithms but with more features and faster\n    training!\n</p>\n<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n        <p\n            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n            Old code</p>\n        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\nimport tensorflow_decision_forests as tfdf\n\ntf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\nmodel = tfdf.keras.RandomForestModel(label=\"l\")\nmodel.fit(tf_ds)\n</pre>\n    </div>\n    <div style=\"width: 5px;\"></div>\n    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n        <p\n            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n            New code</p>\n        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\nimport ydf\n\nmodel = ydf.RandomForestLearner(label=\"l\").train(ds)\n</pre>\n    </div>\n</div>\n<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n        guide</a>)</p>\n"},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1757933798.257935      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# 모델 훈련","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.RandomForestModel(max_depth=5, num_trees=100)\nmodel.fit(x=train_ds,  validation_data=valid_ds)\n# validation_data 가 없으면 훈련만 진행, 있으면 검증까지 함","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:38.424448Z","iopub.execute_input":"2025-09-15T10:56:38.424701Z","iopub.status.idle":"2025-09-15T10:56:45.469487Z","shell.execute_reply.started":"2025-09-15T10:56:38.424684Z","shell.execute_reply":"2025-09-15T10:56:45.468740Z"}},"outputs":[{"name":"stdout","text":"Use /tmp/tmpgdz8onhk as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:04.251759. Found 166 examples.\nReading validation dataset...\nNum validation examples: tf.Tensor(42, shape=(), dtype=int32)\nValidation dataset read in 0:00:00.850065. Found 42 examples.\nTraining model...\nModel trained in 0:00:00.053334\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1757933803.873598      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757933803.874291      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757933803.874339      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757933803.875086      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757933803.875104      36 kernel.cc:402] Number of examples: 166\nI0000 00:00:1757933803.875372      36 kernel.cc:802] Training dataset:\nNumber of records: 166\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0284976 min:0.0015 max:0.1371 sd:0.0215818\n\t1: \"Freq_10\" NUMERICAL mean:0.207663 min:0.0113 max:0.7106 sd:0.131239\n\t2: \"Freq_11\" NUMERICAL mean:0.235482 min:0.0289 max:0.7342 sd:0.131675\n\t3: \"Freq_12\" NUMERICAL mean:0.25421 min:0.0236 max:0.706 sd:0.139511\n\t4: \"Freq_13\" NUMERICAL mean:0.276758 min:0.0184 max:0.7131 sd:0.142933\n\t5: \"Freq_14\" NUMERICAL mean:0.303746 min:0.0273 max:0.997 sd:0.164171\n\t6: \"Freq_15\" NUMERICAL mean:0.32954 min:0.0031 max:1 sd:0.206203\n\t7: \"Freq_16\" NUMERICAL mean:0.385273 min:0.0162 max:0.9988 sd:0.234817\n\t8: \"Freq_17\" NUMERICAL mean:0.419958 min:0.0349 max:1 sd:0.265881\n\t9: \"Freq_18\" NUMERICAL mean:0.455828 min:0.0375 max:1 sd:0.266535\n\t10: \"Freq_19\" NUMERICAL mean:0.512122 min:0.0494 max:1 sd:0.265274\n\t11: \"Freq_2\" NUMERICAL mean:0.0380157 min:0.0006 max:0.1632 sd:0.0312229\n\t12: \"Freq_20\" NUMERICAL mean:0.569985 min:0.0656 max:1 sd:0.267564\n\t13: \"Freq_21\" NUMERICAL mean:0.611049 min:0.0512 max:1 sd:0.262704\n\t14: \"Freq_22\" NUMERICAL mean:0.630669 min:0.0219 max:1 sd:0.258455\n\t15: \"Freq_23\" NUMERICAL mean:0.659513 min:0.0563 max:1 sd:0.242312\n\t16: \"Freq_24\" NUMERICAL mean:0.684749 min:0.0239 max:1 sd:0.226152\n\t17: \"Freq_25\" NUMERICAL mean:0.679126 min:0.024 max:1 sd:0.239933\n\t18: \"Freq_26\" NUMERICAL mean:0.702274 min:0.0921 max:1 sd:0.234785\n\t19: \"Freq_27\" NUMERICAL mean:0.710316 min:0.0481 max:1 sd:0.239496\n\t20: \"Freq_28\" NUMERICAL mean:0.697636 min:0.0284 max:1 sd:0.234767\n\t21: \"Freq_29\" NUMERICAL mean:0.641038 min:0.0716 max:1 sd:0.241104\n\t22: \"Freq_3\" NUMERICAL mean:0.0424837 min:0.0015 max:0.1997 sd:0.0341389\n\t23: \"Freq_30\" NUMERICAL mean:0.581766 min:0.0613 max:1 sd:0.224125\n\t24: \"Freq_31\" NUMERICAL mean:0.509236 min:0.0482 max:0.9657 sd:0.215199\n\t25: \"Freq_32\" NUMERICAL mean:0.445961 min:0.0404 max:0.9306 sd:0.216064\n\t26: \"Freq_33\" NUMERICAL mean:0.421405 min:0.0477 max:0.9708 sd:0.20674\n\t27: \"Freq_34\" NUMERICAL mean:0.403545 min:0.0212 max:0.9647 sd:0.226669\n\t28: \"Freq_35\" NUMERICAL mean:0.38808 min:0.0223 max:1 sd:0.258106\n\t29: \"Freq_36\" NUMERICAL mean:0.374693 min:0.008 max:1 sd:0.263132\n\t30: \"Freq_37\" NUMERICAL mean:0.355412 min:0.0351 max:0.9497 sd:0.234956\n\t31: \"Freq_38\" NUMERICAL mean:0.327139 min:0.0383 max:0.9303 sd:0.202399\n\t32: \"Freq_39\" NUMERICAL mean:0.309716 min:0.0371 max:0.9857 sd:0.189018\n\t33: \"Freq_4\" NUMERICAL mean:0.0532633 min:0.0058 max:0.2604 sd:0.0392494\n\t34: \"Freq_40\" NUMERICAL mean:0.301911 min:0.0117 max:0.9297 sd:0.168413\n\t35: \"Freq_41\" NUMERICAL mean:0.286581 min:0.0386 max:0.8995 sd:0.164736\n\t36: \"Freq_42\" NUMERICAL mean:0.278442 min:0.0056 max:0.8246 sd:0.166243\n\t37: \"Freq_43\" NUMERICAL mean:0.243308 min:0 max:0.6163 sd:0.132555\n\t38: \"Freq_44\" NUMERICAL mean:0.209707 min:0 max:0.5772 sd:0.129218\n\t39: \"Freq_45\" NUMERICAL mean:0.196255 min:0 max:0.7034 sd:0.153064\n\t40: \"Freq_46\" NUMERICAL mean:0.158394 min:0 max:0.6214 sd:0.132463\n\t41: \"Freq_47\" NUMERICAL mean:0.118681 min:0 max:0.4331 sd:0.0841214\n\t42: \"Freq_48\" NUMERICAL mean:0.0876693 min:0 max:0.2945 sd:0.0599173\n\t43: \"Freq_49\" NUMERICAL mean:0.0509518 min:0 max:0.1981 sd:0.0358126\n\t44: \"Freq_5\" NUMERICAL mean:0.0734861 min:0.0067 max:0.3225 sd:0.053059\n\t45: \"Freq_50\" NUMERICAL mean:0.0200078 min:0 max:0.0825 sd:0.0141441\n\t46: \"Freq_51\" NUMERICAL mean:0.0158271 min:0 max:0.0798 sd:0.0107626\n\t47: \"Freq_52\" NUMERICAL mean:0.0135886 min:0.0022 max:0.0459 sd:0.00897481\n\t48: \"Freq_53\" NUMERICAL mean:0.0103404 min:0.001 max:0.0361 sd:0.00646685\n\t49: \"Freq_54\" NUMERICAL mean:0.0104711 min:0.001 max:0.0344 sd:0.00689071\n\t50: \"Freq_55\" NUMERICAL mean:0.00916145 min:0.0011 max:0.0447 sd:0.00731711\n\t51: \"Freq_56\" NUMERICAL mean:0.0080247 min:0.0004 max:0.0394 sd:0.00531318\n\t52: \"Freq_57\" NUMERICAL mean:0.00771928 min:0.0003 max:0.0355 sd:0.00578405\n\t53: \"Freq_58\" NUMERICAL mean:0.00784639 min:0.0003 max:0.044 sd:0.00653784\n\t54: \"Freq_59\" NUMERICAL mean:0.00776205 min:0.0002 max:0.0364 sd:0.006092\n\t55: \"Freq_6\" NUMERICAL mean:0.10596 min:0.0116 max:0.3823 sd:0.0598741\n\t56: \"Freq_60\" NUMERICAL mean:0.00635301 min:0.0006 max:0.0439 sd:0.00509994\n\t57: \"Freq_7\" NUMERICAL mean:0.125851 min:0.0033 max:0.3729 sd:0.0625094\n\t58: \"Freq_8\" NUMERICAL mean:0.135856 min:0.0057 max:0.459 sd:0.0813619\n\t59: \"Freq_9\" NUMERICAL mean:0.176359 min:0.0075 max:0.6828 sd:0.112137\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757933803.875473      36 kernel.cc:807] Collect validation dataset\nI0000 00:00:1757933803.875758      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757933803.875771      36 kernel.cc:402] Number of examples: 42\nI0000 00:00:1757933803.875863      36 kernel.cc:813] Validation dataset:\nNumber of records: 42\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0317976 min:0.0072 max:0.1313 sd:0.0274871\n\t1: \"Freq_10\" NUMERICAL mean:0.210614 min:0.0193 max:0.6609 sd:0.144795\n\t2: \"Freq_11\" NUMERICAL mean:0.238112 min:0.0548 max:0.6675 sd:0.135134\n\t3: \"Freq_12\" NUMERICAL mean:0.234457 min:0.0273 max:0.6552 sd:0.139506\n\t4: \"Freq_13\" NUMERICAL mean:0.259657 min:0.0513 max:0.6919 sd:0.130202\n\t5: \"Freq_14\" NUMERICAL mean:0.268198 min:0.0645 max:0.7797 sd:0.160602\n\t6: \"Freq_15\" NUMERICAL mean:0.283293 min:0.0346 max:0.7464 sd:0.195513\n\t7: \"Freq_16\" NUMERICAL mean:0.351664 min:0.0717 max:0.9444 sd:0.218934\n\t8: \"Freq_17\" NUMERICAL mean:0.400271 min:0.0903 max:1 sd:0.250894\n\t9: \"Freq_18\" NUMERICAL mean:0.438445 min:0.0689 max:1 sd:0.236815\n\t10: \"Freq_19\" NUMERICAL mean:0.475919 min:0.1397 max:0.9407 sd:0.221044\n\t11: \"Freq_2\" NUMERICAL mean:0.0401 min:0.0027 max:0.2339 sd:0.0386996\n\t12: \"Freq_20\" NUMERICAL mean:0.535624 min:0.1426 max:0.9367 sd:0.236875\n\t13: \"Freq_21\" NUMERICAL mean:0.601198 min:0.2109 max:1 sd:0.234007\n\t14: \"Freq_22\" NUMERICAL mean:0.599002 min:0.1973 max:1 sd:0.240599\n\t15: \"Freq_23\" NUMERICAL mean:0.597419 min:0.0671 max:1 sd:0.27079\n\t16: \"Freq_24\" NUMERICAL mean:0.624852 min:0.0248 max:1 sd:0.277115\n\t17: \"Freq_25\" NUMERICAL mean:0.66079 min:0.0747 max:1 sd:0.260503\n\t18: \"Freq_26\" NUMERICAL mean:0.69035 min:0.1969 max:1 sd:0.243682\n\t19: \"Freq_27\" NUMERICAL mean:0.669898 min:0.2234 max:1 sd:0.263477\n\t20: \"Freq_28\" NUMERICAL mean:0.67975 min:0.1911 max:1 sd:0.243272\n\t21: \"Freq_29\" NUMERICAL mean:0.646169 min:0.0144 max:1 sd:0.233878\n\t22: \"Freq_3\" NUMERICAL mean:0.0491619 min:0.0026 max:0.3059 sd:0.0513436\n\t23: \"Freq_30\" NUMERICAL mean:0.577617 min:0.2028 max:1 sd:0.204014\n\t24: \"Freq_31\" NUMERICAL mean:0.485662 min:0.1307 max:0.892 sd:0.205457\n\t25: \"Freq_32\" NUMERICAL mean:0.411686 min:0.0877 max:0.842 sd:0.196605\n\t26: \"Freq_33\" NUMERICAL mean:0.400679 min:0.0863 max:1 sd:0.202283\n\t27: \"Freq_34\" NUMERICAL mean:0.402 min:0.0431 max:0.9536 sd:0.245911\n\t28: \"Freq_35\" NUMERICAL mean:0.410321 min:0.0488 max:0.9017 sd:0.259333\n\t29: \"Freq_36\" NUMERICAL mean:0.424981 min:0.0271 max:1 sd:0.26104\n\t30: \"Freq_37\" NUMERICAL mean:0.396986 min:0.0429 max:0.9123 sd:0.253199\n\t31: \"Freq_38\" NUMERICAL mean:0.389136 min:0.0603 max:1 sd:0.242014\n\t32: \"Freq_39\" NUMERICAL mean:0.389369 min:0.1003 max:0.8849 sd:0.221471\n\t33: \"Freq_4\" NUMERICAL mean:0.0563786 min:0.0061 max:0.4264 sd:0.0676254\n\t34: \"Freq_40\" NUMERICAL mean:0.347948 min:0.0227 max:0.8979 sd:0.208631\n\t35: \"Freq_41\" NUMERICAL mean:0.29981 min:0.036 max:0.7751 sd:0.192106\n\t36: \"Freq_42\" NUMERICAL mean:0.277707 min:0.0431 max:0.7988 sd:0.176297\n\t37: \"Freq_43\" NUMERICAL mean:0.259321 min:0.0516 max:0.7733 sd:0.15989\n\t38: \"Freq_44\" NUMERICAL mean:0.231338 min:0.0337 max:0.7762 sd:0.145587\n\t39: \"Freq_45\" NUMERICAL mean:0.201093 min:0.0046 max:0.6009 sd:0.143861\n\t40: \"Freq_46\" NUMERICAL mean:0.169474 min:0.008 max:0.7292 sd:0.13772\n\t41: \"Freq_47\" NUMERICAL mean:0.137362 min:0.0408 max:0.5522 sd:0.094956\n\t42: \"Freq_48\" NUMERICAL mean:0.106264 min:0.0052 max:0.3339 sd:0.0688159\n\t43: \"Freq_49\" NUMERICAL mean:0.0557905 min:0.0106 max:0.1608 sd:0.0358235\n\t44: \"Freq_5\" NUMERICAL mean:0.0819857 min:0.0121 max:0.401 sd:0.0634443\n\t45: \"Freq_50\" NUMERICAL mean:0.022069 min:0.0045 max:0.0499 sd:0.0112357\n\t46: \"Freq_51\" NUMERICAL mean:0.0170238 min:0.0015 max:0.1004 sd:0.0158669\n\t47: \"Freq_52\" NUMERICAL mean:0.0127548 min:0.0008 max:0.0709 sd:0.0117699\n\t48: \"Freq_53\" NUMERICAL mean:0.0121667 min:0.0005 max:0.039 sd:0.00881648\n\t49: \"Freq_54\" NUMERICAL mean:0.0127976 min:0.0018 max:0.0352 sd:0.00840922\n\t50: \"Freq_55\" NUMERICAL mean:0.0098 min:0.0006 max:0.0252 sd:0.0059706\n\t51: \"Freq_56\" NUMERICAL mean:0.009 min:0.0007 max:0.0326 sd:0.00705904\n\t52: \"Freq_57\" NUMERICAL mean:0.00821905 min:0.0011 max:0.023 sd:0.00570452\n\t53: \"Freq_58\" NUMERICAL mean:0.00835476 min:0.0009 max:0.0244 sd:0.00609591\n\t54: \"Freq_59\" NUMERICAL mean:0.00865 min:0.0001 max:0.0255 sd:0.00640362\n\t55: \"Freq_6\" NUMERICAL mean:0.0990762 min:0.0102 max:0.2583 sd:0.0548727\n\t56: \"Freq_60\" NUMERICAL mean:0.00711667 min:0.0016 max:0.0231 sd:0.00463516\n\t57: \"Freq_7\" NUMERICAL mean:0.105526 min:0.0182 max:0.2495 sd:0.0551607\n\t58: \"Freq_8\" NUMERICAL mean:0.130621 min:0.0055 max:0.4566 sd:0.0977287\n\t59: \"Freq_9\" NUMERICAL mean:0.1845 min:0.0381 max:0.6587 sd:0.139009\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757933803.875933      36 kernel.cc:818] Configure learner\nI0000 00:00:1757933803.876189      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 100\n  decision_tree {\n    max_depth: 5\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757933803.876553      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmpgdz8onhk/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757933803.876735     119 kernel.cc:895] Train model\nI0000 00:00:1757933803.877539     119 random_forest.cc:427] Training random forest on 166 example(s) and 60 feature(s).\nI0000 00:00:1757933803.883002     124 random_forest.cc:811] Training of tree  1/100 (tree index:1) done accuracy:0.754386 logloss:8.85283\nI0000 00:00:1757933803.884015     125 random_forest.cc:811] Training of tree  11/100 (tree index:9) done accuracy:0.751515 logloss:2.30569\nI0000 00:00:1757933803.885186     125 random_forest.cc:811] Training of tree  21/100 (tree index:21) done accuracy:0.753012 logloss:0.828426\nI0000 00:00:1757933803.886174     124 random_forest.cc:811] Training of tree  31/100 (tree index:30) done accuracy:0.801205 logloss:0.61668\nI0000 00:00:1757933803.887265     125 random_forest.cc:811] Training of tree  41/100 (tree index:40) done accuracy:0.813253 logloss:0.628085\nI0000 00:00:1757933803.888369     127 random_forest.cc:811] Training of tree  51/100 (tree index:50) done accuracy:0.801205 logloss:0.433901\nI0000 00:00:1757933803.889422     126 random_forest.cc:811] Training of tree  61/100 (tree index:60) done accuracy:0.795181 logloss:0.433368\nI0000 00:00:1757933803.890503     126 random_forest.cc:811] Training of tree  71/100 (tree index:70) done accuracy:0.783133 logloss:0.439132\nI0000 00:00:1757933803.891502     125 random_forest.cc:811] Training of tree  81/100 (tree index:78) done accuracy:0.795181 logloss:0.435618\nI0000 00:00:1757933803.892721     127 random_forest.cc:811] Training of tree  91/100 (tree index:91) done accuracy:0.777108 logloss:0.43648\nI0000 00:00:1757933803.893654     124 random_forest.cc:811] Training of tree  100/100 (tree index:99) done accuracy:0.789157 logloss:0.428116\nI0000 00:00:1757933803.893729     119 random_forest.cc:891] Final OOB metrics: accuracy:0.789157 logloss:0.428116\nI0000 00:00:1757933803.894399     119 kernel.cc:926] Export model in log directory: /tmp/tmpgdz8onhk with prefix 102db69f769840d1\nI0000 00:00:1757933803.896119     119 kernel.cc:944] Save model in resources\nI0000 00:00:1757933803.899716      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 166\nNumber of predictions (with weights): 166\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.789157  CI95[W][0.730354 0.840035]\nLogLoss: : 0.428116\nErrorRate: : 0.210843\n\nDefault Accuracy: : 0.53012\nDefault LogLoss: : 0.691332\nDefault ErrorRate: : 0.46988\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  75  13\n2  22  56\nTotal: 166\n\n\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1757933803.916954      36 decision_forest.cc:761] Model loaded with 100 root(s), 1832 node(s), and 60 input feature(s).\nI0000 00:00:1757933803.920157      36 abstract_model.cc:1404] Engine \"RandomForestOptPred\" built\n","output_type":"stream"},{"name":"stdout","text":"Model compiled.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7eed19a1bfd0>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# 모델 검증","metadata":{}},{"cell_type":"code","source":"train_evaluation = model.evaluate(x=train_ds, return_dict=True)\nprint(\"train_data_evaluation:\")\nfor name, value in train_evaluation.items():\n    print(f\"- {name}: {value:.4f}\")\n\nvalid_evaluation = model.evaluate(x=valid_ds, return_dict=True)\nprint(\"valid_data_evaluation:\")\nfor name, value in train_evaluation.items():\n    print(f\"- {name}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:56:45.471283Z","iopub.execute_input":"2025-09-15T10:56:45.471589Z","iopub.status.idle":"2025-09-15T10:56:45.801570Z","shell.execute_reply.started":"2025-09-15T10:56:45.471571Z","shell.execute_reply":"2025-09-15T10:56:45.800862Z"}},"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 157ms/step - loss: 0.0000e+00\ntrain_data_evaluation:\n- loss: 0.0000\n1/1 [==============================] - 0s 154ms/step - loss: 0.0000e+00\nvalid_data_evaluation:\n- loss: 0.0000\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"손실율이 0 으로 과적합이 되어버림....\n교차검증을 해보려고 함","metadata":{}},{"cell_type":"markdown","source":"# 교차 검증","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport os\n\nimport logging\nlogging.getLogger(\"absl\").setLevel(logging.ERROR)\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # 0=모두 출력, 1=INFO 무시, 2=INFO/WARNING 무시, 3=ERROR만 출력\nos.environ[\"YDF_CPP_LOG_LEVEL\"] = \"0\"    # Yggdrasil 로그 억제\n\n# 데이터 전처리\ndf_train1 = pd.read_csv('/kaggle/input/connectionist-bench-mines-and-rocks/Sonar.csv')\n\n# 레이블을 0과 1로 변환\ndf_train1['Label'] = df_train1['Label'].apply(lambda x: 0 if x == 'R' else 1)\n\n# K-Fold 설정\nkf = KFold(n_splits=5, shuffle=True)\nall_fold_scores = []\n\n# 교차 검증 루프\nfor train_index, val_index in kf.split(df_train1):\n    # Pandas DataFrame 분할\n    train_df = df_train1.iloc[train_index]\n    val_df = df_train1.iloc[val_index]\n\n    # TFDF 모델 정의 및 훈련\n    model = tfdf.keras.RandomForestModel(verbose=0)\n    \n    # Pandas DataFrame을 TFDF 데이터셋으로 변환\n    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label='Label')\n    val_ds = tfdf.keras.pd_dataframe_to_tf_dataset(val_df, label='Label')\n\n    # 모델 컴파일 및 훈련\n    model.compile(metrics=[\"accuracy\"])\n    model.fit(x=train_ds, verbose=0)\n\n    # 검증 데이터로 성능 평가\n    evaluation = model.evaluate(x=val_ds, return_dict=True, verbose=0)\n    all_fold_scores.append(evaluation['accuracy'])\n\n# 모든 폴드의 평균 성능 계산\nprint(f\"Mean Accuracy: {np.mean(all_fold_scores)}\")\n\n\ntrain_evaluation = model.evaluate(x=train_ds, return_dict=True)\nprint(\"train_data_evaluation:\")\nfor name, value in train_evaluation.items():\n    print(f\"- {name}: {value:.4f}\")\n\nvalid_evaluation = model.evaluate(x=valid_ds, return_dict=True)\nprint(\"valid_data_evaluation:\")\nfor name, value in train_evaluation.items():\n    print(f\"- {name}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T11:05:18.323662Z","iopub.execute_input":"2025-09-15T11:05:18.324207Z","iopub.status.idle":"2025-09-15T11:05:28.273067Z","shell.execute_reply.started":"2025-09-15T11:05:18.324186Z","shell.execute_reply":"2025-09-15T11:05:28.272527Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1757934319.254485      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757934319.254528      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757934319.254539      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757934319.254811      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757934319.254831      36 kernel.cc:402] Number of examples: 166\nI0000 00:00:1757934319.254990      36 kernel.cc:802] Training dataset:\nNumber of records: 166\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0288127 min:0.0025 max:0.1371 sd:0.0220563\n\t1: \"Freq_10\" NUMERICAL mean:0.206354 min:0.0113 max:0.7106 sd:0.132122\n\t2: \"Freq_11\" NUMERICAL mean:0.237583 min:0.0357 max:0.7342 sd:0.134836\n\t3: \"Freq_12\" NUMERICAL mean:0.254217 min:0.0236 max:0.706 sd:0.146837\n\t4: \"Freq_13\" NUMERICAL mean:0.279575 min:0.0184 max:0.7131 sd:0.145786\n\t5: \"Freq_14\" NUMERICAL mean:0.303309 min:0.0273 max:0.8198 sd:0.162182\n\t6: \"Freq_15\" NUMERICAL mean:0.327445 min:0.0031 max:1 sd:0.207449\n\t7: \"Freq_16\" NUMERICAL mean:0.383569 min:0.0162 max:0.9988 sd:0.242684\n\t8: \"Freq_17\" NUMERICAL mean:0.424157 min:0.0349 max:1 sd:0.271023\n\t9: \"Freq_18\" NUMERICAL mean:0.464277 min:0.0689 max:1 sd:0.263113\n\t10: \"Freq_19\" NUMERICAL mean:0.520263 min:0.0494 max:1 sd:0.254566\n\t11: \"Freq_2\" NUMERICAL mean:0.0375633 min:0.0006 max:0.2339 sd:0.0332009\n\t12: \"Freq_20\" NUMERICAL mean:0.572417 min:0.0656 max:1 sd:0.25332\n\t13: \"Freq_21\" NUMERICAL mean:0.616837 min:0.0512 max:1 sd:0.252019\n\t14: \"Freq_22\" NUMERICAL mean:0.636831 min:0.1127 max:1 sd:0.245304\n\t15: \"Freq_23\" NUMERICAL mean:0.659823 min:0.0563 max:1 sd:0.238501\n\t16: \"Freq_24\" NUMERICAL mean:0.690468 min:0.0239 max:1 sd:0.229036\n\t17: \"Freq_25\" NUMERICAL mean:0.69011 min:0.1348 max:1 sd:0.224389\n\t18: \"Freq_26\" NUMERICAL mean:0.70753 min:0.0921 max:1 sd:0.229264\n\t19: \"Freq_27\" NUMERICAL mean:0.70954 min:0.0481 max:1 sd:0.248251\n\t20: \"Freq_28\" NUMERICAL mean:0.696916 min:0.0284 max:1 sd:0.246113\n\t21: \"Freq_29\" NUMERICAL mean:0.64677 min:0.0144 max:1 sd:0.246694\n\t22: \"Freq_3\" NUMERICAL mean:0.044044 min:0.0015 max:0.3059 sd:0.0391119\n\t23: \"Freq_30\" NUMERICAL mean:0.573508 min:0.0613 max:1 sd:0.221945\n\t24: \"Freq_31\" NUMERICAL mean:0.492541 min:0.0482 max:0.9657 sd:0.208792\n\t25: \"Freq_32\" NUMERICAL mean:0.427287 min:0.0404 max:0.9306 sd:0.207992\n\t26: \"Freq_33\" NUMERICAL mean:0.401205 min:0.0507 max:1 sd:0.200643\n\t27: \"Freq_34\" NUMERICAL mean:0.378508 min:0.0212 max:0.9647 sd:0.220915\n\t28: \"Freq_35\" NUMERICAL mean:0.366549 min:0.0223 max:1 sd:0.238961\n\t29: \"Freq_36\" NUMERICAL mean:0.356487 min:0.008 max:1 sd:0.242579\n\t30: \"Freq_37\" NUMERICAL mean:0.339756 min:0.0351 max:0.9497 sd:0.221329\n\t31: \"Freq_38\" NUMERICAL mean:0.315539 min:0.0383 max:1 sd:0.199413\n\t32: \"Freq_39\" NUMERICAL mean:0.303623 min:0.0371 max:0.9857 sd:0.193426\n\t33: \"Freq_4\" NUMERICAL mean:0.0537133 min:0.0058 max:0.4264 sd:0.048064\n\t34: \"Freq_40\" NUMERICAL mean:0.296606 min:0.0117 max:0.9297 sd:0.175275\n\t35: \"Freq_41\" NUMERICAL mean:0.272615 min:0.036 max:0.8995 sd:0.167835\n\t36: \"Freq_42\" NUMERICAL mean:0.261819 min:0.0056 max:0.8246 sd:0.164262\n\t37: \"Freq_43\" NUMERICAL mean:0.232816 min:0 max:0.7733 sd:0.132248\n\t38: \"Freq_44\" NUMERICAL mean:0.20506 min:0 max:0.7762 sd:0.127535\n\t39: \"Freq_45\" NUMERICAL mean:0.185796 min:0 max:0.7034 sd:0.144683\n\t40: \"Freq_46\" NUMERICAL mean:0.149305 min:0 max:0.6214 sd:0.125123\n\t41: \"Freq_47\" NUMERICAL mean:0.117205 min:0 max:0.4331 sd:0.0844699\n\t42: \"Freq_48\" NUMERICAL mean:0.0883614 min:0 max:0.2945 sd:0.0607011\n\t43: \"Freq_49\" NUMERICAL mean:0.050209 min:0 max:0.1981 sd:0.0360236\n\t44: \"Freq_5\" NUMERICAL mean:0.0748657 min:0.0067 max:0.401 sd:0.0559761\n\t45: \"Freq_50\" NUMERICAL mean:0.0194325 min:0 max:0.0779 sd:0.0136286\n\t46: \"Freq_51\" NUMERICAL mean:0.0151783 min:0 max:0.0798 sd:0.0105552\n\t47: \"Freq_52\" NUMERICAL mean:0.012897 min:0.0013 max:0.0444 sd:0.00831104\n\t48: \"Freq_53\" NUMERICAL mean:0.0101645 min:0.0005 max:0.039 sd:0.00660688\n\t49: \"Freq_54\" NUMERICAL mean:0.0105361 min:0.001 max:0.0344 sd:0.00693081\n\t50: \"Freq_55\" NUMERICAL mean:0.00886145 min:0.0011 max:0.0376 sd:0.00641835\n\t51: \"Freq_56\" NUMERICAL mean:0.00802952 min:0.0004 max:0.0277 sd:0.00535237\n\t52: \"Freq_57\" NUMERICAL mean:0.00736084 min:0.0003 max:0.0316 sd:0.00543623\n\t53: \"Freq_58\" NUMERICAL mean:0.00728133 min:0.0003 max:0.0377 sd:0.00555137\n\t54: \"Freq_59\" NUMERICAL mean:0.00761988 min:0.0001 max:0.0364 sd:0.00557023\n\t55: \"Freq_6\" NUMERICAL mean:0.105029 min:0.0102 max:0.3823 sd:0.0609891\n\t56: \"Freq_60\" NUMERICAL mean:0.00600964 min:0.0006 max:0.0231 sd:0.00413902\n\t57: \"Freq_7\" NUMERICAL mean:0.122417 min:0.0033 max:0.3729 sd:0.0597092\n\t58: \"Freq_8\" NUMERICAL mean:0.133602 min:0.0055 max:0.4223 sd:0.0787221\n\t59: \"Freq_9\" NUMERICAL mean:0.173499 min:0.0075 max:0.5744 sd:0.108543\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757934319.255081      36 kernel.cc:818] Configure learner\nI0000 00:00:1757934319.255369      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 16\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757934319.255487      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmpw_40uddb/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757934319.255684    1155 kernel.cc:895] Train model\nI0000 00:00:1757934319.256968    1155 random_forest.cc:427] Training random forest on 166 example(s) and 60 feature(s).\nI0000 00:00:1757934319.258154    1160 random_forest.cc:811] Training of tree  1/300 (tree index:1) done accuracy:0.77193 logloss:8.22048\nI0000 00:00:1757934319.259599    1162 random_forest.cc:811] Training of tree  11/300 (tree index:10) done accuracy:0.727273 logloss:2.58855\nI0000 00:00:1757934319.261292    1163 random_forest.cc:811] Training of tree  21/300 (tree index:19) done accuracy:0.73494 logloss:0.691977\nI0000 00:00:1757934319.262743    1162 random_forest.cc:811] Training of tree  31/300 (tree index:30) done accuracy:0.740964 logloss:0.465752\nI0000 00:00:1757934319.264161    1162 random_forest.cc:811] Training of tree  41/300 (tree index:39) done accuracy:0.746988 logloss:0.460485\nI0000 00:00:1757934319.265873    1161 random_forest.cc:811] Training of tree  51/300 (tree index:50) done accuracy:0.771084 logloss:0.454938\nI0000 00:00:1757934319.267351    1160 random_forest.cc:811] Training of tree  61/300 (tree index:61) done accuracy:0.777108 logloss:0.447509\nI0000 00:00:1757934319.269009    1163 random_forest.cc:811] Training of tree  71/300 (tree index:70) done accuracy:0.76506 logloss:0.445885\nI0000 00:00:1757934319.270333    1160 random_forest.cc:811] Training of tree  81/300 (tree index:80) done accuracy:0.759036 logloss:0.441248\nI0000 00:00:1757934319.271959    1162 random_forest.cc:811] Training of tree  91/300 (tree index:90) done accuracy:0.783133 logloss:0.437691\nI0000 00:00:1757934319.273317    1161 random_forest.cc:811] Training of tree  101/300 (tree index:101) done accuracy:0.777108 logloss:0.440913\nI0000 00:00:1757934319.274933    1163 random_forest.cc:811] Training of tree  111/300 (tree index:110) done accuracy:0.795181 logloss:0.437936\nI0000 00:00:1757934319.276192    1163 random_forest.cc:811] Training of tree  121/300 (tree index:120) done accuracy:0.795181 logloss:0.434824\nI0000 00:00:1757934319.277795    1160 random_forest.cc:811] Training of tree  131/300 (tree index:130) done accuracy:0.795181 logloss:0.436745\nI0000 00:00:1757934319.279211    1162 random_forest.cc:811] Training of tree  141/300 (tree index:140) done accuracy:0.795181 logloss:0.43829\nI0000 00:00:1757934319.280646    1160 random_forest.cc:811] Training of tree  151/300 (tree index:150) done accuracy:0.789157 logloss:0.440367\nI0000 00:00:1757934319.282201    1161 random_forest.cc:811] Training of tree  161/300 (tree index:160) done accuracy:0.807229 logloss:0.443661\nI0000 00:00:1757934319.283626    1162 random_forest.cc:811] Training of tree  171/300 (tree index:170) done accuracy:0.813253 logloss:0.443392\nI0000 00:00:1757934319.285192    1163 random_forest.cc:811] Training of tree  181/300 (tree index:180) done accuracy:0.801205 logloss:0.444247\nI0000 00:00:1757934319.286736    1161 random_forest.cc:811] Training of tree  191/300 (tree index:190) done accuracy:0.801205 logloss:0.444306\nI0000 00:00:1757934319.288172    1163 random_forest.cc:811] Training of tree  201/300 (tree index:200) done accuracy:0.801205 logloss:0.441167\nI0000 00:00:1757934319.289696    1160 random_forest.cc:811] Training of tree  211/300 (tree index:210) done accuracy:0.813253 logloss:0.442024\nI0000 00:00:1757934319.291111    1162 random_forest.cc:811] Training of tree  221/300 (tree index:219) done accuracy:0.813253 logloss:0.443844\nI0000 00:00:1757934319.292583    1160 random_forest.cc:811] Training of tree  231/300 (tree index:231) done accuracy:0.807229 logloss:0.441586\nI0000 00:00:1757934319.294061    1161 random_forest.cc:811] Training of tree  241/300 (tree index:239) done accuracy:0.801205 logloss:0.442115\nI0000 00:00:1757934319.295763    1161 random_forest.cc:811] Training of tree  251/300 (tree index:251) done accuracy:0.813253 logloss:0.440797\nI0000 00:00:1757934319.297312    1160 random_forest.cc:811] Training of tree  261/300 (tree index:260) done accuracy:0.813253 logloss:0.440565\nI0000 00:00:1757934319.298695    1161 random_forest.cc:811] Training of tree  271/300 (tree index:270) done accuracy:0.807229 logloss:0.441528\nI0000 00:00:1757934319.300380    1161 random_forest.cc:811] Training of tree  281/300 (tree index:281) done accuracy:0.819277 logloss:0.439467\nI0000 00:00:1757934319.301706    1162 random_forest.cc:811] Training of tree  291/300 (tree index:290) done accuracy:0.813253 logloss:0.440123\nI0000 00:00:1757934319.303191    1160 random_forest.cc:811] Training of tree  300/300 (tree index:299) done accuracy:0.813253 logloss:0.439618\nI0000 00:00:1757934319.303277    1155 random_forest.cc:891] Final OOB metrics: accuracy:0.813253 logloss:0.439618\nI0000 00:00:1757934319.305835    1155 kernel.cc:926] Export model in log directory: /tmp/tmpw_40uddb with prefix 57700a09db5c4125\nI0000 00:00:1757934319.310728    1155 kernel.cc:944] Save model in resources\nI0000 00:00:1757934319.312352      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 166\nNumber of predictions (with weights): 166\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.813253  CI95[W][0.756399 0.861496]\nLogLoss: : 0.439618\nErrorRate: : 0.186747\n\nDefault Accuracy: : 0.518072\nDefault LogLoss: : 0.692494\nDefault ErrorRate: : 0.481928\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  60  20\n2  11  75\nTotal: 166\n\n\nI0000 00:00:1757934319.346051      36 decision_forest.cc:761] Model loaded with 300 root(s), 7808 node(s), and 60 input feature(s).\nI0000 00:00:1757934319.346085      36 abstract_model.cc:1404] Engine \"RandomForestOptPred\" built\nI0000 00:00:1757934321.044740      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757934321.044781      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757934321.044790      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757934321.045058      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757934321.045079      36 kernel.cc:402] Number of examples: 166\nI0000 00:00:1757934321.045265      36 kernel.cc:802] Training dataset:\nNumber of records: 166\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0309018 min:0.0015 max:0.1371 sd:0.024588\n\t1: \"Freq_10\" NUMERICAL mean:0.210789 min:0.0113 max:0.7106 sd:0.139827\n\t2: \"Freq_11\" NUMERICAL mean:0.238369 min:0.0289 max:0.7342 sd:0.134294\n\t3: \"Freq_12\" NUMERICAL mean:0.253858 min:0.0236 max:0.706 sd:0.137506\n\t4: \"Freq_13\" NUMERICAL mean:0.276113 min:0.0252 max:0.7131 sd:0.142155\n\t5: \"Freq_14\" NUMERICAL mean:0.29631 min:0.0273 max:0.997 sd:0.167635\n\t6: \"Freq_15\" NUMERICAL mean:0.315398 min:0.0031 max:1 sd:0.207109\n\t7: \"Freq_16\" NUMERICAL mean:0.369218 min:0.0162 max:0.9988 sd:0.224473\n\t8: \"Freq_17\" NUMERICAL mean:0.401465 min:0.0349 max:1 sd:0.250148\n\t9: \"Freq_18\" NUMERICAL mean:0.435355 min:0.0375 max:1 sd:0.251138\n\t10: \"Freq_19\" NUMERICAL mean:0.492601 min:0.0494 max:0.9843 sd:0.25243\n\t11: \"Freq_2\" NUMERICAL mean:0.0396319 min:0.0017 max:0.2339 sd:0.0335321\n\t12: \"Freq_20\" NUMERICAL mean:0.560483 min:0.074 max:1 sd:0.260428\n\t13: \"Freq_21\" NUMERICAL mean:0.611725 min:0.0512 max:1 sd:0.253498\n\t14: \"Freq_22\" NUMERICAL mean:0.626183 min:0.0219 max:1 sd:0.249097\n\t15: \"Freq_23\" NUMERICAL mean:0.648044 min:0.0563 max:1 sd:0.251399\n\t16: \"Freq_24\" NUMERICAL mean:0.670782 min:0.0239 max:1 sd:0.241059\n\t17: \"Freq_25\" NUMERICAL mean:0.674086 min:0.024 max:1 sd:0.247517\n\t18: \"Freq_26\" NUMERICAL mean:0.704702 min:0.0921 max:1 sd:0.235082\n\t19: \"Freq_27\" NUMERICAL mean:0.704389 min:0.0481 max:1 sd:0.236566\n\t20: \"Freq_28\" NUMERICAL mean:0.696783 min:0.0876 max:1 sd:0.220269\n\t21: \"Freq_29\" NUMERICAL mean:0.645702 min:0.0144 max:1 sd:0.228869\n\t22: \"Freq_3\" NUMERICAL mean:0.0441572 min:0.0015 max:0.3059 sd:0.0376561\n\t23: \"Freq_30\" NUMERICAL mean:0.58847 min:0.0613 max:1 sd:0.22195\n\t24: \"Freq_31\" NUMERICAL mean:0.508737 min:0.0482 max:0.9656 sd:0.216291\n\t25: \"Freq_32\" NUMERICAL mean:0.438327 min:0.0587 max:0.9168 sd:0.211656\n\t26: \"Freq_33\" NUMERICAL mean:0.424148 min:0.0477 max:1 sd:0.210577\n\t27: \"Freq_34\" NUMERICAL mean:0.422023 min:0.0212 max:0.9647 sd:0.234402\n\t28: \"Freq_35\" NUMERICAL mean:0.412521 min:0.0223 max:1 sd:0.265081\n\t29: \"Freq_36\" NUMERICAL mean:0.400373 min:0.008 max:1 sd:0.271177\n\t30: \"Freq_37\" NUMERICAL mean:0.376184 min:0.0351 max:0.9497 sd:0.244409\n\t31: \"Freq_38\" NUMERICAL mean:0.350067 min:0.0461 max:1 sd:0.215625\n\t32: \"Freq_39\" NUMERICAL mean:0.340393 min:0.0407 max:0.8849 sd:0.190473\n\t33: \"Freq_4\" NUMERICAL mean:0.0544235 min:0.0058 max:0.4264 sd:0.0466845\n\t34: \"Freq_40\" NUMERICAL mean:0.319899 min:0.0202 max:0.8979 sd:0.169072\n\t35: \"Freq_41\" NUMERICAL mean:0.298987 min:0.0386 max:0.7751 sd:0.161041\n\t36: \"Freq_42\" NUMERICAL mean:0.285381 min:0.0328 max:0.8246 sd:0.165272\n\t37: \"Freq_43\" NUMERICAL mean:0.255079 min:0 max:0.7733 sd:0.137407\n\t38: \"Freq_44\" NUMERICAL mean:0.221601 min:0 max:0.7762 sd:0.13433\n\t39: \"Freq_45\" NUMERICAL mean:0.201537 min:0 max:0.6448 sd:0.150635\n\t40: \"Freq_46\" NUMERICAL mean:0.165615 min:0 max:0.7292 sd:0.13349\n\t41: \"Freq_47\" NUMERICAL mean:0.122642 min:0 max:0.5522 sd:0.0847886\n\t42: \"Freq_48\" NUMERICAL mean:0.0903958 min:0 max:0.3339 sd:0.0616259\n\t43: \"Freq_49\" NUMERICAL mean:0.052238 min:0 max:0.1794 sd:0.0349682\n\t44: \"Freq_5\" NUMERICAL mean:0.0750169 min:0.0067 max:0.401 sd:0.0544782\n\t45: \"Freq_50\" NUMERICAL mean:0.0202614 min:0 max:0.0825 sd:0.0129149\n\t46: \"Freq_51\" NUMERICAL mean:0.0158446 min:0 max:0.1004 sd:0.0125739\n\t47: \"Freq_52\" NUMERICAL mean:0.0135169 min:0.0008 max:0.0709 sd:0.0100942\n\t48: \"Freq_53\" NUMERICAL mean:0.0109313 min:0.0005 max:0.039 sd:0.00712505\n\t49: \"Freq_54\" NUMERICAL mean:0.0111283 min:0.001 max:0.0352 sd:0.007719\n\t50: \"Freq_55\" NUMERICAL mean:0.00939398 min:0.0006 max:0.0447 sd:0.00726923\n\t51: \"Freq_56\" NUMERICAL mean:0.00858012 min:0.0004 max:0.0394 sd:0.0060168\n\t52: \"Freq_57\" NUMERICAL mean:0.00811024 min:0.0003 max:0.0355 sd:0.00593669\n\t53: \"Freq_58\" NUMERICAL mean:0.00839157 min:0.0006 max:0.044 sd:0.0066498\n\t54: \"Freq_59\" NUMERICAL mean:0.00825964 min:0.0001 max:0.0364 sd:0.00644028\n\t55: \"Freq_6\" NUMERICAL mean:0.10435 min:0.0102 max:0.3823 sd:0.0590225\n\t56: \"Freq_60\" NUMERICAL mean:0.00655422 min:0.0006 max:0.0439 sd:0.00526331\n\t57: \"Freq_7\" NUMERICAL mean:0.120851 min:0.013 max:0.3729 sd:0.0630336\n\t58: \"Freq_8\" NUMERICAL mean:0.137181 min:0.0055 max:0.459 sd:0.090413\n\t59: \"Freq_9\" NUMERICAL mean:0.183547 min:0.0117 max:0.6828 sd:0.125599\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757934321.045368      36 kernel.cc:818] Configure learner\nI0000 00:00:1757934321.045606      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 16\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757934321.045703      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmpkij6d95a/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757934321.045823    1194 kernel.cc:895] Train model\nI0000 00:00:1757934321.046680    1194 random_forest.cc:427] Training random forest on 166 example(s) and 60 feature(s).\nI0000 00:00:1757934321.048348    1202 random_forest.cc:811] Training of tree  1/300 (tree index:0) done accuracy:0.706897 logloss:10.5645\nI0000 00:00:1757934321.049641    1202 random_forest.cc:811] Training of tree  11/300 (tree index:9) done accuracy:0.690909 logloss:2.55691\nI0000 00:00:1757934321.051313    1201 random_forest.cc:811] Training of tree  21/300 (tree index:22) done accuracy:0.759036 logloss:0.651864\nI0000 00:00:1757934321.052658    1202 random_forest.cc:811] Training of tree  31/300 (tree index:30) done accuracy:0.819277 logloss:0.428577\nI0000 00:00:1757934321.054148    1201 random_forest.cc:811] Training of tree  41/300 (tree index:40) done accuracy:0.837349 logloss:0.403376\nI0000 00:00:1757934321.055796    1200 random_forest.cc:811] Training of tree  51/300 (tree index:51) done accuracy:0.819277 logloss:0.396143\nI0000 00:00:1757934321.057050    1200 random_forest.cc:811] Training of tree  61/300 (tree index:59) done accuracy:0.789157 logloss:0.395072\nI0000 00:00:1757934321.058868    1199 random_forest.cc:811] Training of tree  71/300 (tree index:70) done accuracy:0.801205 logloss:0.390089\nI0000 00:00:1757934321.060184    1202 random_forest.cc:811] Training of tree  81/300 (tree index:80) done accuracy:0.825301 logloss:0.387032\nI0000 00:00:1757934321.061734    1199 random_forest.cc:811] Training of tree  91/300 (tree index:90) done accuracy:0.825301 logloss:0.390537\nI0000 00:00:1757934321.063077    1201 random_forest.cc:811] Training of tree  101/300 (tree index:100) done accuracy:0.807229 logloss:0.397458\nI0000 00:00:1757934321.064726    1199 random_forest.cc:811] Training of tree  111/300 (tree index:110) done accuracy:0.813253 logloss:0.394908\nI0000 00:00:1757934321.066004    1202 random_forest.cc:811] Training of tree  121/300 (tree index:120) done accuracy:0.801205 logloss:0.391368\nI0000 00:00:1757934321.067694    1200 random_forest.cc:811] Training of tree  131/300 (tree index:130) done accuracy:0.819277 logloss:0.394764\nI0000 00:00:1757934321.069110    1199 random_forest.cc:811] Training of tree  141/300 (tree index:140) done accuracy:0.825301 logloss:0.394569\nI0000 00:00:1757934321.070708    1202 random_forest.cc:811] Training of tree  151/300 (tree index:151) done accuracy:0.831325 logloss:0.397676\nI0000 00:00:1757934321.072034    1199 random_forest.cc:811] Training of tree  161/300 (tree index:160) done accuracy:0.831325 logloss:0.398541\nI0000 00:00:1757934321.073593    1201 random_forest.cc:811] Training of tree  171/300 (tree index:169) done accuracy:0.837349 logloss:0.400553\nI0000 00:00:1757934321.075091    1200 random_forest.cc:811] Training of tree  181/300 (tree index:180) done accuracy:0.831325 logloss:0.402642\nI0000 00:00:1757934321.076565    1201 random_forest.cc:811] Training of tree  191/300 (tree index:190) done accuracy:0.837349 logloss:0.402969\nI0000 00:00:1757934321.078353    1200 random_forest.cc:811] Training of tree  201/300 (tree index:200) done accuracy:0.831325 logloss:0.401815\nI0000 00:00:1757934321.079681    1199 random_forest.cc:811] Training of tree  211/300 (tree index:211) done accuracy:0.831325 logloss:0.405082\nI0000 00:00:1757934321.081237    1201 random_forest.cc:811] Training of tree  221/300 (tree index:220) done accuracy:0.819277 logloss:0.404727\nI0000 00:00:1757934321.082698    1199 random_forest.cc:811] Training of tree  231/300 (tree index:231) done accuracy:0.819277 logloss:0.409524\nI0000 00:00:1757934321.084274    1200 random_forest.cc:811] Training of tree  241/300 (tree index:241) done accuracy:0.813253 logloss:0.413092\nI0000 00:00:1757934321.085758    1202 random_forest.cc:811] Training of tree  251/300 (tree index:251) done accuracy:0.819277 logloss:0.410133\nI0000 00:00:1757934321.087169    1200 random_forest.cc:811] Training of tree  261/300 (tree index:260) done accuracy:0.825301 logloss:0.408531\nI0000 00:00:1757934321.088715    1199 random_forest.cc:811] Training of tree  271/300 (tree index:270) done accuracy:0.807229 logloss:0.408403\nI0000 00:00:1757934321.090102    1202 random_forest.cc:811] Training of tree  281/300 (tree index:280) done accuracy:0.807229 logloss:0.410558\nI0000 00:00:1757934321.091802    1199 random_forest.cc:811] Training of tree  291/300 (tree index:290) done accuracy:0.807229 logloss:0.4095\nI0000 00:00:1757934321.092971    1199 random_forest.cc:811] Training of tree  300/300 (tree index:299) done accuracy:0.813253 logloss:0.409786\nI0000 00:00:1757934321.093046    1194 random_forest.cc:891] Final OOB metrics: accuracy:0.813253 logloss:0.409786\nI0000 00:00:1757934321.095564    1194 kernel.cc:926] Export model in log directory: /tmp/tmpkij6d95a with prefix 2adbab6fe5c44234\nI0000 00:00:1757934321.100178    1194 kernel.cc:944] Save model in resources\nI0000 00:00:1757934321.101720      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 166\nNumber of predictions (with weights): 166\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.813253  CI95[W][0.756399 0.861496]\nLogLoss: : 0.409786\nErrorRate: : 0.186747\n\nDefault Accuracy: : 0.566265\nDefault LogLoss: : 0.684339\nDefault ErrorRate: : 0.433735\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  49  23\n2   8  86\nTotal: 166\n\n\nI0000 00:00:1757934321.135144      36 decision_forest.cc:761] Model loaded with 300 root(s), 7652 node(s), and 60 input feature(s).\nI0000 00:00:1757934322.849390      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757934322.849425      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757934322.849434      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757934322.849676      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757934322.849696      36 kernel.cc:402] Number of examples: 166\nI0000 00:00:1757934322.849855      36 kernel.cc:802] Training dataset:\nNumber of records: 166\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0283072 min:0.0015 max:0.1371 sd:0.0229012\n\t1: \"Freq_10\" NUMERICAL mean:0.208831 min:0.0113 max:0.7106 sd:0.14042\n\t2: \"Freq_11\" NUMERICAL mean:0.233927 min:0.0289 max:0.7342 sd:0.134706\n\t3: \"Freq_12\" NUMERICAL mean:0.247566 min:0.0236 max:0.706 sd:0.138412\n\t4: \"Freq_13\" NUMERICAL mean:0.273252 min:0.0184 max:0.7131 sd:0.137806\n\t5: \"Freq_14\" NUMERICAL mean:0.295807 min:0.0336 max:0.997 sd:0.162356\n\t6: \"Freq_15\" NUMERICAL mean:0.31861 min:0.0031 max:1 sd:0.200697\n\t7: \"Freq_16\" NUMERICAL mean:0.377434 min:0.0162 max:0.9988 sd:0.227196\n\t8: \"Freq_17\" NUMERICAL mean:0.412746 min:0.0349 max:1 sd:0.261773\n\t9: \"Freq_18\" NUMERICAL mean:0.444766 min:0.0375 max:1 sd:0.262273\n\t10: \"Freq_19\" NUMERICAL mean:0.495638 min:0.0494 max:1 sd:0.262912\n\t11: \"Freq_2\" NUMERICAL mean:0.0386633 min:0.0006 max:0.1632 sd:0.0320735\n\t12: \"Freq_20\" NUMERICAL mean:0.559126 min:0.0656 max:1 sd:0.262169\n\t13: \"Freq_21\" NUMERICAL mean:0.605196 min:0.0512 max:1 sd:0.259095\n\t14: \"Freq_22\" NUMERICAL mean:0.615898 min:0.0219 max:1 sd:0.261827\n\t15: \"Freq_23\" NUMERICAL mean:0.635796 min:0.061 max:1 sd:0.260009\n\t16: \"Freq_24\" NUMERICAL mean:0.655294 min:0.0248 max:1 sd:0.245847\n\t17: \"Freq_25\" NUMERICAL mean:0.656303 min:0.024 max:1 sd:0.252346\n\t18: \"Freq_26\" NUMERICAL mean:0.682529 min:0.0921 max:1 sd:0.240282\n\t19: \"Freq_27\" NUMERICAL mean:0.687975 min:0.0481 max:1 sd:0.251392\n\t20: \"Freq_28\" NUMERICAL mean:0.69034 min:0.0284 max:1 sd:0.241262\n\t21: \"Freq_29\" NUMERICAL mean:0.645754 min:0.0144 max:1 sd:0.238206\n\t22: \"Freq_3\" NUMERICAL mean:0.0438723 min:0.0015 max:0.1997 sd:0.0357506\n\t23: \"Freq_30\" NUMERICAL mean:0.593317 min:0.1594 max:1 sd:0.211381\n\t24: \"Freq_31\" NUMERICAL mean:0.513964 min:0.1 max:0.9657 sd:0.210844\n\t25: \"Freq_32\" NUMERICAL mean:0.449254 min:0.0404 max:0.9306 sd:0.21449\n\t26: \"Freq_33\" NUMERICAL mean:0.43018 min:0.0477 max:1 sd:0.206177\n\t27: \"Freq_34\" NUMERICAL mean:0.41658 min:0.0212 max:0.9647 sd:0.236164\n\t28: \"Freq_35\" NUMERICAL mean:0.404792 min:0.0223 max:1 sd:0.266783\n\t29: \"Freq_36\" NUMERICAL mean:0.393268 min:0.0271 max:1 sd:0.262548\n\t30: \"Freq_37\" NUMERICAL mean:0.364313 min:0.0351 max:0.9497 sd:0.23921\n\t31: \"Freq_38\" NUMERICAL mean:0.348318 min:0.0383 max:1 sd:0.215068\n\t32: \"Freq_39\" NUMERICAL mean:0.337192 min:0.0371 max:0.9857 sd:0.201133\n\t33: \"Freq_4\" NUMERICAL mean:0.0539482 min:0.0061 max:0.2604 sd:0.0405083\n\t34: \"Freq_40\" NUMERICAL mean:0.316373 min:0.0117 max:0.9297 sd:0.180281\n\t35: \"Freq_41\" NUMERICAL mean:0.296128 min:0.036 max:0.8995 sd:0.171995\n\t36: \"Freq_42\" NUMERICAL mean:0.285289 min:0.0056 max:0.8246 sd:0.170704\n\t37: \"Freq_43\" NUMERICAL mean:0.248747 min:0 max:0.7517 sd:0.140905\n\t38: \"Freq_44\" NUMERICAL mean:0.210463 min:0 max:0.5772 sd:0.131577\n\t39: \"Freq_45\" NUMERICAL mean:0.197827 min:0 max:0.7034 sd:0.157001\n\t40: \"Freq_46\" NUMERICAL mean:0.162422 min:0 max:0.7292 sd:0.141736\n\t41: \"Freq_47\" NUMERICAL mean:0.124558 min:0 max:0.5522 sd:0.0912722\n\t42: \"Freq_48\" NUMERICAL mean:0.0942753 min:0 max:0.3339 sd:0.0645567\n\t43: \"Freq_49\" NUMERICAL mean:0.0524434 min:0 max:0.1981 sd:0.0372683\n\t44: \"Freq_5\" NUMERICAL mean:0.074856 min:0.0067 max:0.3225 sd:0.0516526\n\t45: \"Freq_50\" NUMERICAL mean:0.0209789 min:0 max:0.0825 sd:0.0145781\n\t46: \"Freq_51\" NUMERICAL mean:0.0168446 min:0 max:0.1004 sd:0.0126984\n\t47: \"Freq_52\" NUMERICAL mean:0.0136922 min:0.0008 max:0.0709 sd:0.0100408\n\t48: \"Freq_53\" NUMERICAL mean:0.0110681 min:0.0014 max:0.039 sd:0.00726604\n\t49: \"Freq_54\" NUMERICAL mean:0.0116187 min:0.001 max:0.0352 sd:0.00751394\n\t50: \"Freq_55\" NUMERICAL mean:0.0098 min:0.0006 max:0.0447 sd:0.00744093\n\t51: \"Freq_56\" NUMERICAL mean:0.00855 min:0.0004 max:0.0394 sd:0.00577231\n\t52: \"Freq_57\" NUMERICAL mean:0.0081512 min:0.0003 max:0.0355 sd:0.00589838\n\t53: \"Freq_58\" NUMERICAL mean:0.00809578 min:0.0003 max:0.044 sd:0.0064555\n\t54: \"Freq_59\" NUMERICAL mean:0.00821446 min:0.0002 max:0.0364 sd:0.00642039\n\t55: \"Freq_6\" NUMERICAL mean:0.103434 min:0.0102 max:0.3823 sd:0.0574309\n\t56: \"Freq_60\" NUMERICAL mean:0.00666024 min:0.0006 max:0.0439 sd:0.00512211\n\t57: \"Freq_7\" NUMERICAL mean:0.121149 min:0.0033 max:0.3729 sd:0.0630703\n\t58: \"Freq_8\" NUMERICAL mean:0.13679 min:0.0057 max:0.459 sd:0.0885086\n\t59: \"Freq_9\" NUMERICAL mean:0.180652 min:0.0075 max:0.6828 sd:0.124979\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757934322.849940      36 kernel.cc:818] Configure learner\nI0000 00:00:1757934322.850227      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 16\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757934322.850371      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmp2v87ku4v/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757934322.850519    1233 kernel.cc:895] Train model\nI0000 00:00:1757934322.851550    1233 random_forest.cc:427] Training random forest on 166 example(s) and 60 feature(s).\nI0000 00:00:1757934322.853085    1240 random_forest.cc:811] Training of tree  1/300 (tree index:3) done accuracy:0.606557 logloss:14.1811\nI0000 00:00:1757934322.854424    1241 random_forest.cc:811] Training of tree  11/300 (tree index:9) done accuracy:0.69697 logloss:2.37403\nI0000 00:00:1757934322.856025    1239 random_forest.cc:811] Training of tree  21/300 (tree index:20) done accuracy:0.753012 logloss:0.865418\nI0000 00:00:1757934322.857355    1241 random_forest.cc:811] Training of tree  31/300 (tree index:29) done accuracy:0.771084 logloss:0.43353\nI0000 00:00:1757934322.858887    1239 random_forest.cc:811] Training of tree  41/300 (tree index:40) done accuracy:0.783133 logloss:0.420724\nI0000 00:00:1757934322.860466    1238 random_forest.cc:811] Training of tree  51/300 (tree index:51) done accuracy:0.807229 logloss:0.417954\nI0000 00:00:1757934322.861796    1240 random_forest.cc:811] Training of tree  61/300 (tree index:60) done accuracy:0.837349 logloss:0.410881\nI0000 00:00:1757934322.863395    1238 random_forest.cc:811] Training of tree  71/300 (tree index:70) done accuracy:0.831325 logloss:0.408808\nI0000 00:00:1757934322.864751    1240 random_forest.cc:811] Training of tree  81/300 (tree index:80) done accuracy:0.837349 logloss:0.4157\nI0000 00:00:1757934322.866252    1238 random_forest.cc:811] Training of tree  91/300 (tree index:91) done accuracy:0.819277 logloss:0.418474\nI0000 00:00:1757934322.867614    1240 random_forest.cc:811] Training of tree  101/300 (tree index:100) done accuracy:0.837349 logloss:0.416637\nI0000 00:00:1757934322.869140    1241 random_forest.cc:811] Training of tree  111/300 (tree index:109) done accuracy:0.813253 logloss:0.415842\nI0000 00:00:1757934322.870631    1239 random_forest.cc:811] Training of tree  121/300 (tree index:120) done accuracy:0.813253 logloss:0.410282\nI0000 00:00:1757934322.872000    1240 random_forest.cc:811] Training of tree  131/300 (tree index:130) done accuracy:0.819277 logloss:0.40987\nI0000 00:00:1757934322.873602    1239 random_forest.cc:811] Training of tree  141/300 (tree index:140) done accuracy:0.831325 logloss:0.410588\nI0000 00:00:1757934322.874967    1238 random_forest.cc:811] Training of tree  151/300 (tree index:149) done accuracy:0.843373 logloss:0.411482\nI0000 00:00:1757934322.876640    1240 random_forest.cc:811] Training of tree  161/300 (tree index:160) done accuracy:0.843373 logloss:0.411326\nI0000 00:00:1757934322.878047    1239 random_forest.cc:811] Training of tree  171/300 (tree index:170) done accuracy:0.843373 logloss:0.415694\nI0000 00:00:1757934322.879514    1241 random_forest.cc:811] Training of tree  181/300 (tree index:180) done accuracy:0.849398 logloss:0.41365\nI0000 00:00:1757934322.880960    1240 random_forest.cc:811] Training of tree  191/300 (tree index:190) done accuracy:0.843373 logloss:0.410611\nI0000 00:00:1757934322.882607    1241 random_forest.cc:811] Training of tree  201/300 (tree index:199) done accuracy:0.831325 logloss:0.412691\nI0000 00:00:1757934322.884035    1240 random_forest.cc:811] Training of tree  211/300 (tree index:210) done accuracy:0.843373 logloss:0.414457\nI0000 00:00:1757934322.885465    1241 random_forest.cc:811] Training of tree  221/300 (tree index:220) done accuracy:0.843373 logloss:0.414671\nI0000 00:00:1757934322.887141    1239 random_forest.cc:811] Training of tree  231/300 (tree index:231) done accuracy:0.837349 logloss:0.415224\nI0000 00:00:1757934322.888681    1238 random_forest.cc:811] Training of tree  241/300 (tree index:241) done accuracy:0.837349 logloss:0.415682\nI0000 00:00:1757934322.890381    1240 random_forest.cc:811] Training of tree  251/300 (tree index:250) done accuracy:0.843373 logloss:0.414969\nI0000 00:00:1757934322.892713    1240 random_forest.cc:811] Training of tree  261/300 (tree index:261) done accuracy:0.849398 logloss:0.413479\nI0000 00:00:1757934322.896068    1240 random_forest.cc:811] Training of tree  271/300 (tree index:272) done accuracy:0.837349 logloss:0.412688\nI0000 00:00:1757934322.898788    1238 random_forest.cc:811] Training of tree  281/300 (tree index:282) done accuracy:0.855422 logloss:0.412134\nI0000 00:00:1757934322.901008    1240 random_forest.cc:811] Training of tree  291/300 (tree index:291) done accuracy:0.837349 logloss:0.413095\nI0000 00:00:1757934322.902981    1240 random_forest.cc:811] Training of tree  300/300 (tree index:294) done accuracy:0.837349 logloss:0.41283\nI0000 00:00:1757934322.903060    1233 random_forest.cc:891] Final OOB metrics: accuracy:0.837349 logloss:0.41283\nI0000 00:00:1757934322.906599    1233 kernel.cc:926] Export model in log directory: /tmp/tmp2v87ku4v with prefix 5951bfcdb681468e\nI0000 00:00:1757934322.912582    1233 kernel.cc:944] Save model in resources\nI0000 00:00:1757934322.914349      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 166\nNumber of predictions (with weights): 166\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.837349  CI95[W][0.782748 0.882642]\nLogLoss: : 0.41283\nErrorRate: : 0.162651\n\nDefault Accuracy: : 0.536145\nDefault LogLoss: : 0.690532\nDefault ErrorRate: : 0.463855\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  59  18\n2   9  80\nTotal: 166\n\n\nI0000 00:00:1757934322.960518      36 decision_forest.cc:761] Model loaded with 300 root(s), 7646 node(s), and 60 input feature(s).\nI0000 00:00:1757934324.655845      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757934324.655883      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757934324.655891      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757934324.656123      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757934324.656142      36 kernel.cc:402] Number of examples: 167\nI0000 00:00:1757934324.656359      36 kernel.cc:802] Training dataset:\nNumber of records: 167\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0287347 min:0.0015 max:0.1313 sd:0.0222053\n\t1: \"Freq_10\" NUMERICAL mean:0.208354 min:0.0113 max:0.6609 sd:0.125055\n\t2: \"Freq_11\" NUMERICAL mean:0.233596 min:0.0289 max:0.6675 sd:0.124282\n\t3: \"Freq_12\" NUMERICAL mean:0.246313 min:0.0259 max:0.6552 sd:0.135491\n\t4: \"Freq_13\" NUMERICAL mean:0.268654 min:0.0184 max:0.7022 sd:0.138556\n\t5: \"Freq_14\" NUMERICAL mean:0.293572 min:0.0273 max:0.997 sd:0.166164\n\t6: \"Freq_15\" NUMERICAL mean:0.318113 min:0.0092 max:1 sd:0.208849\n\t7: \"Freq_16\" NUMERICAL mean:0.37874 min:0.0572 max:0.9988 sd:0.231561\n\t8: \"Freq_17\" NUMERICAL mean:0.419141 min:0.0349 max:1 sd:0.263266\n\t9: \"Freq_18\" NUMERICAL mean:0.456422 min:0.0375 max:1 sd:0.260599\n\t10: \"Freq_19\" NUMERICAL mean:0.500155 min:0.0494 max:1 sd:0.254441\n\t11: \"Freq_2\" NUMERICAL mean:0.037309 min:0.0006 max:0.2339 sd:0.0317341\n\t12: \"Freq_20\" NUMERICAL mean:0.55331 min:0.0656 max:1 sd:0.267386\n\t13: \"Freq_21\" NUMERICAL mean:0.599001 min:0.0512 max:1 sd:0.262602\n\t14: \"Freq_22\" NUMERICAL mean:0.616384 min:0.0219 max:1 sd:0.258694\n\t15: \"Freq_23\" NUMERICAL mean:0.640947 min:0.0563 max:1 sd:0.253238\n\t16: \"Freq_24\" NUMERICAL mean:0.666378 min:0.0239 max:1 sd:0.24475\n\t17: \"Freq_25\" NUMERICAL mean:0.672733 min:0.024 max:1 sd:0.253355\n\t18: \"Freq_26\" NUMERICAL mean:0.699899 min:0.0921 max:1 sd:0.238566\n\t19: \"Freq_27\" NUMERICAL mean:0.699953 min:0.0481 max:1 sd:0.245087\n\t20: \"Freq_28\" NUMERICAL mean:0.68984 min:0.0284 max:1 sd:0.23835\n\t21: \"Freq_29\" NUMERICAL mean:0.637393 min:0.0144 max:1 sd:0.245005\n\t22: \"Freq_3\" NUMERICAL mean:0.0427425 min:0.0015 max:0.3059 sd:0.0393986\n\t23: \"Freq_30\" NUMERICAL mean:0.573147 min:0.0613 max:1 sd:0.222625\n\t24: \"Freq_31\" NUMERICAL mean:0.504104 min:0.0482 max:0.9657 sd:0.214241\n\t25: \"Freq_32\" NUMERICAL mean:0.447011 min:0.0404 max:0.9306 sd:0.210834\n\t26: \"Freq_33\" NUMERICAL mean:0.420719 min:0.0477 max:1 sd:0.203225\n\t27: \"Freq_34\" NUMERICAL mean:0.404159 min:0.0306 max:0.9536 sd:0.226095\n\t28: \"Freq_35\" NUMERICAL mean:0.395573 min:0.0223 max:1 sd:0.259573\n\t29: \"Freq_36\" NUMERICAL mean:0.399295 min:0.008 max:1 sd:0.272959\n\t30: \"Freq_37\" NUMERICAL mean:0.380698 min:0.0379 max:0.9497 sd:0.252067\n\t31: \"Freq_38\" NUMERICAL mean:0.349178 min:0.0383 max:1 sd:0.223592\n\t32: \"Freq_39\" NUMERICAL mean:0.330368 min:0.0371 max:0.9857 sd:0.208604\n\t33: \"Freq_4\" NUMERICAL mean:0.0524132 min:0.0058 max:0.4264 sd:0.0479338\n\t34: \"Freq_40\" NUMERICAL mean:0.315347 min:0.0117 max:0.9297 sd:0.188438\n\t35: \"Freq_41\" NUMERICAL mean:0.284641 min:0.036 max:0.8995 sd:0.174926\n\t36: \"Freq_42\" NUMERICAL mean:0.271305 min:0.0056 max:0.7988 sd:0.166157\n\t37: \"Freq_43\" NUMERICAL mean:0.242347 min:0 max:0.7733 sd:0.13905\n\t38: \"Freq_44\" NUMERICAL mean:0.213136 min:0 max:0.7762 sd:0.137236\n\t39: \"Freq_45\" NUMERICAL mean:0.195585 min:0 max:0.7034 sd:0.149846\n\t40: \"Freq_46\" NUMERICAL mean:0.159087 min:0 max:0.7292 sd:0.128607\n\t41: \"Freq_47\" NUMERICAL mean:0.119907 min:0 max:0.5522 sd:0.0852762\n\t42: \"Freq_48\" NUMERICAL mean:0.0899838 min:0 max:0.3339 sd:0.063513\n\t43: \"Freq_49\" NUMERICAL mean:0.0512407 min:0 max:0.1981 sd:0.0356294\n\t44: \"Freq_5\" NUMERICAL mean:0.0753976 min:0.0076 max:0.401 sd:0.0564623\n\t45: \"Freq_50\" NUMERICAL mean:0.0202754 min:0 max:0.0825 sd:0.0133019\n\t46: \"Freq_51\" NUMERICAL mean:0.0154168 min:0 max:0.1004 sd:0.0113711\n\t47: \"Freq_52\" NUMERICAL mean:0.0129928 min:0.0008 max:0.0709 sd:0.00960669\n\t48: \"Freq_53\" NUMERICAL mean:0.0105749 min:0.0005 max:0.039 sd:0.00720966\n\t49: \"Freq_54\" NUMERICAL mean:0.0104269 min:0.0011 max:0.0352 sd:0.00684318\n\t50: \"Freq_55\" NUMERICAL mean:0.00906707 min:0.0006 max:0.0447 sd:0.00684666\n\t51: \"Freq_56\" NUMERICAL mean:0.00784731 min:0.0007 max:0.0394 sd:0.00571333\n\t52: \"Freq_57\" NUMERICAL mean:0.0076988 min:0.0003 max:0.0355 sd:0.00558826\n\t53: \"Freq_58\" NUMERICAL mean:0.00780539 min:0.0003 max:0.044 sd:0.00664872\n\t54: \"Freq_59\" NUMERICAL mean:0.00746347 min:0.0001 max:0.0332 sd:0.00584557\n\t55: \"Freq_6\" NUMERICAL mean:0.105905 min:0.0116 max:0.3823 sd:0.0597826\n\t56: \"Freq_60\" NUMERICAL mean:0.00648862 min:0.0011 max:0.0439 sd:0.00505081\n\t57: \"Freq_7\" NUMERICAL mean:0.122108 min:0.0033 max:0.3729 sd:0.0621476\n\t58: \"Freq_8\" NUMERICAL mean:0.129878 min:0.0055 max:0.459 sd:0.0797899\n\t59: \"Freq_9\" NUMERICAL mean:0.174541 min:0.0075 max:0.6828 sd:0.111677\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757934324.656448      36 kernel.cc:818] Configure learner\nI0000 00:00:1757934324.656701      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 16\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757934324.656811      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmpmywk_38i/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757934324.656944    1272 kernel.cc:895] Train model\nI0000 00:00:1757934324.657765    1272 random_forest.cc:427] Training random forest on 167 example(s) and 60 feature(s).\nI0000 00:00:1757934324.659123    1280 random_forest.cc:811] Training of tree  1/300 (tree index:0) done accuracy:0.677966 logloss:11.6073\nI0000 00:00:1757934324.660409    1277 random_forest.cc:811] Training of tree  11/300 (tree index:9) done accuracy:0.730539 logloss:2.95143\nI0000 00:00:1757934324.662048    1279 random_forest.cc:811] Training of tree  21/300 (tree index:20) done accuracy:0.772455 logloss:1.09115\nI0000 00:00:1757934324.663443    1278 random_forest.cc:811] Training of tree  31/300 (tree index:30) done accuracy:0.772455 logloss:0.485301\nI0000 00:00:1757934324.665104    1280 random_forest.cc:811] Training of tree  41/300 (tree index:39) done accuracy:0.766467 logloss:0.462373\nI0000 00:00:1757934324.666734    1279 random_forest.cc:811] Training of tree  51/300 (tree index:50) done accuracy:0.790419 logloss:0.464713\nI0000 00:00:1757934324.668137    1279 random_forest.cc:811] Training of tree  61/300 (tree index:59) done accuracy:0.790419 logloss:0.462652\nI0000 00:00:1757934324.669795    1277 random_forest.cc:811] Training of tree  71/300 (tree index:70) done accuracy:0.784431 logloss:0.451584\nI0000 00:00:1757934324.671137    1279 random_forest.cc:811] Training of tree  81/300 (tree index:80) done accuracy:0.802395 logloss:0.455044\nI0000 00:00:1757934324.672732    1278 random_forest.cc:811] Training of tree  91/300 (tree index:91) done accuracy:0.796407 logloss:0.446603\nI0000 00:00:1757934324.674306    1277 random_forest.cc:811] Training of tree  101/300 (tree index:101) done accuracy:0.802395 logloss:0.445435\nI0000 00:00:1757934324.675957    1278 random_forest.cc:811] Training of tree  111/300 (tree index:110) done accuracy:0.808383 logloss:0.438021\nI0000 00:00:1757934324.677459    1280 random_forest.cc:811] Training of tree  121/300 (tree index:121) done accuracy:0.802395 logloss:0.435562\nI0000 00:00:1757934324.678924    1279 random_forest.cc:811] Training of tree  131/300 (tree index:130) done accuracy:0.802395 logloss:0.436319\nI0000 00:00:1757934324.680683    1277 random_forest.cc:811] Training of tree  141/300 (tree index:140) done accuracy:0.790419 logloss:0.439501\nI0000 00:00:1757934324.681939    1279 random_forest.cc:811] Training of tree  151/300 (tree index:150) done accuracy:0.796407 logloss:0.439503\nI0000 00:00:1757934324.683587    1277 random_forest.cc:811] Training of tree  161/300 (tree index:161) done accuracy:0.790419 logloss:0.435444\nI0000 00:00:1757934324.685090    1279 random_forest.cc:811] Training of tree  171/300 (tree index:170) done accuracy:0.796407 logloss:0.435768\nI0000 00:00:1757934324.686439    1278 random_forest.cc:811] Training of tree  181/300 (tree index:181) done accuracy:0.796407 logloss:0.435217\nI0000 00:00:1757934324.687964    1279 random_forest.cc:811] Training of tree  191/300 (tree index:190) done accuracy:0.790419 logloss:0.43641\nI0000 00:00:1757934324.689284    1279 random_forest.cc:811] Training of tree  201/300 (tree index:200) done accuracy:0.790419 logloss:0.433907\nI0000 00:00:1757934324.690882    1277 random_forest.cc:811] Training of tree  211/300 (tree index:210) done accuracy:0.790419 logloss:0.434675\nI0000 00:00:1757934324.692424    1279 random_forest.cc:811] Training of tree  221/300 (tree index:221) done accuracy:0.796407 logloss:0.434245\nI0000 00:00:1757934324.693882    1277 random_forest.cc:811] Training of tree  231/300 (tree index:230) done accuracy:0.796407 logloss:0.432323\nI0000 00:00:1757934324.695358    1278 random_forest.cc:811] Training of tree  241/300 (tree index:241) done accuracy:0.808383 logloss:0.432803\nI0000 00:00:1757934324.696822    1277 random_forest.cc:811] Training of tree  251/300 (tree index:250) done accuracy:0.802395 logloss:0.433996\nI0000 00:00:1757934324.698229    1279 random_forest.cc:811] Training of tree  261/300 (tree index:260) done accuracy:0.808383 logloss:0.43199\nI0000 00:00:1757934324.699903    1278 random_forest.cc:811] Training of tree  271/300 (tree index:270) done accuracy:0.808383 logloss:0.4299\nI0000 00:00:1757934324.701489    1280 random_forest.cc:811] Training of tree  281/300 (tree index:280) done accuracy:0.808383 logloss:0.429244\nI0000 00:00:1757934324.702899    1280 random_forest.cc:811] Training of tree  291/300 (tree index:290) done accuracy:0.796407 logloss:0.427931\nI0000 00:00:1757934324.704324    1280 random_forest.cc:811] Training of tree  300/300 (tree index:299) done accuracy:0.796407 logloss:0.428896\nI0000 00:00:1757934324.704382    1272 random_forest.cc:891] Final OOB metrics: accuracy:0.796407 logloss:0.428896\nI0000 00:00:1757934324.706938    1272 kernel.cc:926] Export model in log directory: /tmp/tmpmywk_38i with prefix 030fb798bc7f4bae\nI0000 00:00:1757934324.711823    1272 kernel.cc:944] Save model in resources\nI0000 00:00:1757934324.713433      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 167\nNumber of predictions (with weights): 167\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.796407  CI95[W][0.738356 0.846378]\nLogLoss: : 0.428896\nErrorRate: : 0.203593\n\nDefault Accuracy: : 0.502994\nDefault LogLoss: : 0.693129\nDefault ErrorRate: : 0.497006\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  66  17\n2  17  67\nTotal: 167\n\n\nI0000 00:00:1757934324.747730      36 decision_forest.cc:761] Model loaded with 300 root(s), 7822 node(s), and 60 input feature(s).\nI0000 00:00:1757934327.021834      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757934327.021877      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757934327.021886      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757934327.022077      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757934327.022089      36 kernel.cc:402] Number of examples: 167\nI0000 00:00:1757934327.022263      36 kernel.cc:802] Training dataset:\nNumber of records: 167\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0290665 min:0.0015 max:0.1371 sd:0.0227551\n\t1: \"Freq_10\" NUMERICAL mean:0.206974 min:0.0193 max:0.7106 sd:0.132454\n\t2: \"Freq_11\" NUMERICAL mean:0.236601 min:0.0289 max:0.7342 sd:0.13347\n\t3: \"Freq_12\" NUMERICAL mean:0.249183 min:0.0236 max:0.706 sd:0.139994\n\t4: \"Freq_13\" NUMERICAL mean:0.268987 min:0.0184 max:0.7131 sd:0.13836\n\t5: \"Freq_14\" NUMERICAL mean:0.293878 min:0.0273 max:0.997 sd:0.161779\n\t6: \"Freq_15\" NUMERICAL mean:0.321447 min:0.0031 max:0.9137 sd:0.200196\n\t7: \"Freq_16\" NUMERICAL mean:0.38344 min:0.0162 max:0.9751 sd:0.233807\n\t8: \"Freq_17\" NUMERICAL mean:0.422349 min:0.0624 max:1 sd:0.267844\n\t9: \"Freq_18\" NUMERICAL mean:0.460696 min:0.0375 max:1 sd:0.266\n\t10: \"Freq_19\" NUMERICAL mean:0.515366 min:0.0746 max:1 sd:0.261139\n\t11: \"Freq_2\" NUMERICAL mean:0.0390186 min:0.0006 max:0.2339 sd:0.0337556\n\t12: \"Freq_20\" NUMERICAL mean:0.569916 min:0.0656 max:1 sd:0.266026\n\t13: \"Freq_21\" NUMERICAL mean:0.61258 min:0.0521 max:1 sd:0.258208\n\t14: \"Freq_22\" NUMERICAL mean:0.626115 min:0.0219 max:1 sd:0.260353\n\t15: \"Freq_23\" NUMERICAL mean:0.650281 min:0.0563 max:1 sd:0.243484\n\t16: \"Freq_24\" NUMERICAL mean:0.680341 min:0.0239 max:1 sd:0.229962\n\t17: \"Freq_25\" NUMERICAL mean:0.683852 min:0.024 max:1 sd:0.241536\n\t18: \"Freq_26\" NUMERICAL mean:0.704643 min:0.1543 max:1 sd:0.239038\n\t19: \"Freq_27\" NUMERICAL mean:0.708889 min:0.0874 max:1 sd:0.243152\n\t20: \"Freq_28\" NUMERICAL mean:0.696254 min:0.0284 max:1 sd:0.236171\n\t21: \"Freq_29\" NUMERICAL mean:0.634822 min:0.0408 max:1 sd:0.238887\n\t22: \"Freq_3\" NUMERICAL mean:0.0443485 min:0.0024 max:0.3059 sd:0.0395895\n\t23: \"Freq_30\" NUMERICAL mean:0.576272 min:0.0613 max:1 sd:0.22216\n\t24: \"Freq_31\" NUMERICAL mean:0.503042 min:0.0482 max:0.9657 sd:0.216495\n\t25: \"Freq_32\" NUMERICAL mean:0.43331 min:0.0404 max:0.9306 sd:0.217699\n\t26: \"Freq_33\" NUMERICAL mean:0.409871 min:0.0477 max:0.9708 sd:0.208005\n\t27: \"Freq_34\" NUMERICAL mean:0.39494 min:0.0212 max:0.9647 sd:0.232877\n\t28: \"Freq_35\" NUMERICAL mean:0.383457 min:0.0244 max:1 sd:0.258633\n\t29: \"Freq_36\" NUMERICAL mean:0.374789 min:0.008 max:1 sd:0.264277\n\t30: \"Freq_37\" NUMERICAL mean:0.358016 min:0.0351 max:0.9419 sd:0.236312\n\t31: \"Freq_38\" NUMERICAL mean:0.335154 min:0.0383 max:0.948 sd:0.205673\n\t32: \"Freq_39\" NUMERICAL mean:0.317444 min:0.0371 max:0.9857 sd:0.196448\n\t33: \"Freq_4\" NUMERICAL mean:0.0549659 min:0.0058 max:0.4264 sd:0.0483571\n\t34: \"Freq_40\" NUMERICAL mean:0.307804 min:0.0117 max:0.9297 sd:0.1765\n\t35: \"Freq_41\" NUMERICAL mean:0.293889 min:0.036 max:0.8995 sd:0.175857\n\t36: \"Freq_42\" NUMERICAL mean:0.287658 min:0.0056 max:0.8246 sd:0.173501\n\t37: \"Freq_43\" NUMERICAL mean:0.253702 min:0.0308 max:0.7733 sd:0.142232\n\t38: \"Freq_44\" NUMERICAL mean:0.220085 min:0.0255 max:0.7762 sd:0.133243\n\t39: \"Freq_45\" NUMERICAL mean:0.205377 min:0.0095 max:0.7034 sd:0.153154\n\t40: \"Freq_46\" NUMERICAL mean:0.166701 min:0.0025 max:0.7292 sd:0.137721\n\t41: \"Freq_47\" NUMERICAL mean:0.127935 min:0.0073 max:0.5522 sd:0.0873347\n\t42: \"Freq_48\" NUMERICAL mean:0.0940964 min:0.0041 max:0.3339 sd:0.0606149\n\t43: \"Freq_49\" NUMERICAL mean:0.0535078 min:0.0073 max:0.1981 sd:0.0353206\n\t44: \"Freq_5\" NUMERICAL mean:0.0758707 min:0.0067 max:0.401 sd:0.0582703\n\t45: \"Freq_50\" NUMERICAL mean:0.0211683 min:0.0006 max:0.0825 sd:0.0136139\n\t46: \"Freq_51\" NUMERICAL mean:0.0170575 min:0.0015 max:0.1004 sd:0.0124359\n\t47: \"Freq_52\" NUMERICAL mean:0.0140012 min:0.0008 max:0.0709 sd:0.00984161\n\t48: \"Freq_53\" NUMERICAL mean:0.0108072 min:0.0005 max:0.0361 sd:0.00695288\n\t49: \"Freq_54\" NUMERICAL mean:0.010997 min:0.001 max:0.0352 sd:0.00730943\n\t50: \"Freq_55\" NUMERICAL mean:0.00933054 min:0.0006 max:0.0447 sd:0.00729226\n\t51: \"Freq_56\" NUMERICAL mean:0.00810419 min:0.0004 max:0.0394 sd:0.00569969\n\t52: \"Freq_57\" NUMERICAL mean:0.00778084 min:0.0007 max:0.0355 sd:0.0059429\n\t53: \"Freq_58\" NUMERICAL mean:0.00817066 min:0.0003 max:0.044 sd:0.00682592\n\t54: \"Freq_59\" NUMERICAL mean:0.0081509 min:0.0001 max:0.0364 sd:0.00645602\n\t55: \"Freq_6\" NUMERICAL mean:0.104127 min:0.0102 max:0.307 sd:0.0574856\n\t56: \"Freq_60\" NUMERICAL mean:0.00682156 min:0.0006 max:0.0439 sd:0.0053824\n\t57: \"Freq_7\" NUMERICAL mean:0.122204 min:0.0033 max:0.3322 sd:0.0601423\n\t58: \"Freq_8\" NUMERICAL mean:0.136562 min:0.0055 max:0.459 sd:0.0864483\n\t59: \"Freq_9\" NUMERICAL mean:0.177797 min:0.0075 max:0.6828 sd:0.118451\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757934327.022360      36 kernel.cc:818] Configure learner\nI0000 00:00:1757934327.022583      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 16\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757934327.022677      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmpfn_datpn/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757934327.022811    1311 kernel.cc:895] Train model\nI0000 00:00:1757934327.023971    1311 random_forest.cc:427] Training random forest on 167 example(s) and 60 feature(s).\nI0000 00:00:1757934327.025565    1316 random_forest.cc:811] Training of tree  1/300 (tree index:1) done accuracy:0.66129 logloss:12.2083\nI0000 00:00:1757934327.026897    1316 random_forest.cc:811] Training of tree  11/300 (tree index:10) done accuracy:0.754491 logloss:1.88594\nI0000 00:00:1757934327.028455    1319 random_forest.cc:811] Training of tree  21/300 (tree index:20) done accuracy:0.760479 logloss:0.848865\nI0000 00:00:1757934327.030073    1317 random_forest.cc:811] Training of tree  31/300 (tree index:29) done accuracy:0.802395 logloss:0.624726\nI0000 00:00:1757934327.031604    1319 random_forest.cc:811] Training of tree  41/300 (tree index:39) done accuracy:0.784431 logloss:0.421714\nI0000 00:00:1757934327.033195    1317 random_forest.cc:811] Training of tree  51/300 (tree index:49) done accuracy:0.778443 logloss:0.42821\nI0000 00:00:1757934327.034516    1317 random_forest.cc:811] Training of tree  61/300 (tree index:58) done accuracy:0.820359 logloss:0.410726\nI0000 00:00:1757934327.036280    1317 random_forest.cc:811] Training of tree  71/300 (tree index:71) done accuracy:0.820359 logloss:0.415449\nI0000 00:00:1757934327.037609    1318 random_forest.cc:811] Training of tree  81/300 (tree index:80) done accuracy:0.844311 logloss:0.40513\nI0000 00:00:1757934327.039128    1316 random_forest.cc:811] Training of tree  91/300 (tree index:89) done accuracy:0.826347 logloss:0.403082\nI0000 00:00:1757934327.040783    1318 random_forest.cc:811] Training of tree  101/300 (tree index:100) done accuracy:0.844311 logloss:0.396613\nI0000 00:00:1757934327.042258    1319 random_forest.cc:811] Training of tree  111/300 (tree index:110) done accuracy:0.838323 logloss:0.395942\nI0000 00:00:1757934327.043918    1318 random_forest.cc:811] Training of tree  121/300 (tree index:120) done accuracy:0.844311 logloss:0.395429\nI0000 00:00:1757934327.045488    1317 random_forest.cc:811] Training of tree  131/300 (tree index:131) done accuracy:0.838323 logloss:0.395545\nI0000 00:00:1757934327.046919    1318 random_forest.cc:811] Training of tree  141/300 (tree index:140) done accuracy:0.832335 logloss:0.394531\nI0000 00:00:1757934327.048227    1316 random_forest.cc:811] Training of tree  151/300 (tree index:150) done accuracy:0.838323 logloss:0.396569\nI0000 00:00:1757934327.049802    1317 random_forest.cc:811] Training of tree  161/300 (tree index:159) done accuracy:0.844311 logloss:0.396032\nI0000 00:00:1757934327.051399    1316 random_forest.cc:811] Training of tree  171/300 (tree index:170) done accuracy:0.850299 logloss:0.393153\nI0000 00:00:1757934327.052978    1317 random_forest.cc:811] Training of tree  181/300 (tree index:180) done accuracy:0.856287 logloss:0.393091\nI0000 00:00:1757934327.054339    1316 random_forest.cc:811] Training of tree  191/300 (tree index:189) done accuracy:0.844311 logloss:0.393531\nI0000 00:00:1757934327.055979    1317 random_forest.cc:811] Training of tree  201/300 (tree index:200) done accuracy:0.850299 logloss:0.3946\nI0000 00:00:1757934327.057307    1319 random_forest.cc:811] Training of tree  211/300 (tree index:210) done accuracy:0.844311 logloss:0.396911\nI0000 00:00:1757934327.058906    1318 random_forest.cc:811] Training of tree  221/300 (tree index:220) done accuracy:0.832335 logloss:0.398542\nI0000 00:00:1757934327.060328    1316 random_forest.cc:811] Training of tree  231/300 (tree index:231) done accuracy:0.832335 logloss:0.399798\nI0000 00:00:1757934327.061681    1318 random_forest.cc:811] Training of tree  241/300 (tree index:240) done accuracy:0.838323 logloss:0.40343\nI0000 00:00:1757934327.063460    1316 random_forest.cc:811] Training of tree  251/300 (tree index:251) done accuracy:0.832335 logloss:0.403757\nI0000 00:00:1757934327.064753    1317 random_forest.cc:811] Training of tree  261/300 (tree index:260) done accuracy:0.826347 logloss:0.402659\nI0000 00:00:1757934327.066378    1316 random_forest.cc:811] Training of tree  271/300 (tree index:270) done accuracy:0.826347 logloss:0.402937\nI0000 00:00:1757934327.067837    1317 random_forest.cc:811] Training of tree  281/300 (tree index:280) done accuracy:0.838323 logloss:0.404543\nI0000 00:00:1757934327.069451    1319 random_forest.cc:811] Training of tree  291/300 (tree index:290) done accuracy:0.832335 logloss:0.404281\nI0000 00:00:1757934327.070707    1318 random_forest.cc:811] Training of tree  300/300 (tree index:299) done accuracy:0.826347 logloss:0.404679\nI0000 00:00:1757934327.070771    1311 random_forest.cc:891] Final OOB metrics: accuracy:0.826347 logloss:0.404679\nI0000 00:00:1757934327.073377    1311 kernel.cc:926] Export model in log directory: /tmp/tmpfn_datpn with prefix 1efbb599d3c74c18\nI0000 00:00:1757934327.078185    1311 kernel.cc:944] Save model in resources\nI0000 00:00:1757934327.079900      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 167\nNumber of predictions (with weights): 167\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.826347  CI95[W][0.770865 0.872896]\nLogLoss: : 0.404679\nErrorRate: : 0.173653\n\nDefault Accuracy: : 0.54491\nDefault LogLoss: : 0.689108\nDefault ErrorRate: : 0.45509\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  56  20\n2   9  82\nTotal: 167\n\n\nI0000 00:00:1757934327.113834      36 decision_forest.cc:761] Model loaded with 300 root(s), 7646 node(s), and 60 input feature(s).\n","output_type":"stream"},{"name":"stdout","text":"Mean Accuracy: 0.8268292665481567\n1/1 [==============================] - 0s 183ms/step - loss: 0.0000e+00 - accuracy: 0.9940\ntrain_data_evaluation:\n- loss: 0.0000\n- accuracy: 0.9940\n1/1 [==============================] - 0s 185ms/step - loss: 0.0000e+00 - accuracy: 0.0714\nvalid_data_evaluation:\n- loss: 0.0000\n- accuracy: 0.9940\n","output_type":"stream"}],"execution_count":26}]}