{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8670316,"sourceType":"datasetVersion","datasetId":4831849}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:08:59.176649Z","iopub.execute_input":"2025-09-15T05:08:59.177167Z","iopub.status.idle":"2025-09-15T05:08:59.289884Z","shell.execute_reply.started":"2025-09-15T05:08:59.177142Z","shell.execute_reply":"2025-09-15T05:08:59.289144Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 데이터 불러오기","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/connectionist-bench-mines-and-rocks/Sonar.csv')\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:06:26.894392Z","iopub.execute_input":"2025-09-15T05:06:26.895181Z","iopub.status.idle":"2025-09-15T05:06:26.939919Z","shell.execute_reply.started":"2025-09-15T05:06:26.895159Z","shell.execute_reply":"2025-09-15T05:06:26.939284Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Freq_1  Freq_2  Freq_3  Freq_4  Freq_5  Freq_6  Freq_7  Freq_8  Freq_9  \\\n0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n\n   Freq_10  ...  Freq_52  Freq_53  Freq_54  Freq_55  Freq_56  Freq_57  \\\n0   0.2111  ...   0.0027   0.0065   0.0159   0.0072   0.0167   0.0180   \n1   0.2872  ...   0.0084   0.0089   0.0048   0.0094   0.0191   0.0140   \n2   0.6194  ...   0.0232   0.0166   0.0095   0.0180   0.0244   0.0316   \n3   0.1264  ...   0.0121   0.0036   0.0150   0.0085   0.0073   0.0050   \n4   0.4459  ...   0.0031   0.0054   0.0105   0.0110   0.0015   0.0072   \n\n   Freq_58  Freq_59  Freq_60  Label  \n0   0.0084   0.0090   0.0032      R  \n1   0.0049   0.0052   0.0044      R  \n2   0.0164   0.0095   0.0078      R  \n3   0.0044   0.0040   0.0117      R  \n4   0.0048   0.0107   0.0094      R  \n\n[5 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Freq_1</th>\n      <th>Freq_2</th>\n      <th>Freq_3</th>\n      <th>Freq_4</th>\n      <th>Freq_5</th>\n      <th>Freq_6</th>\n      <th>Freq_7</th>\n      <th>Freq_8</th>\n      <th>Freq_9</th>\n      <th>Freq_10</th>\n      <th>...</th>\n      <th>Freq_52</th>\n      <th>Freq_53</th>\n      <th>Freq_54</th>\n      <th>Freq_55</th>\n      <th>Freq_56</th>\n      <th>Freq_57</th>\n      <th>Freq_58</th>\n      <th>Freq_59</th>\n      <th>Freq_60</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# 데이터 탐색","metadata":{}},{"cell_type":"code","source":"df_train.info()\ndf_train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:06:29.150757Z","iopub.execute_input":"2025-09-15T05:06:29.151013Z","iopub.status.idle":"2025-09-15T05:06:29.255236Z","shell.execute_reply.started":"2025-09-15T05:06:29.150996Z","shell.execute_reply":"2025-09-15T05:06:29.254443Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 208 entries, 0 to 207\nData columns (total 61 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   Freq_1   208 non-null    float64\n 1   Freq_2   208 non-null    float64\n 2   Freq_3   208 non-null    float64\n 3   Freq_4   208 non-null    float64\n 4   Freq_5   208 non-null    float64\n 5   Freq_6   208 non-null    float64\n 6   Freq_7   208 non-null    float64\n 7   Freq_8   208 non-null    float64\n 8   Freq_9   208 non-null    float64\n 9   Freq_10  208 non-null    float64\n 10  Freq_11  208 non-null    float64\n 11  Freq_12  208 non-null    float64\n 12  Freq_13  208 non-null    float64\n 13  Freq_14  208 non-null    float64\n 14  Freq_15  208 non-null    float64\n 15  Freq_16  208 non-null    float64\n 16  Freq_17  208 non-null    float64\n 17  Freq_18  208 non-null    float64\n 18  Freq_19  208 non-null    float64\n 19  Freq_20  208 non-null    float64\n 20  Freq_21  208 non-null    float64\n 21  Freq_22  208 non-null    float64\n 22  Freq_23  208 non-null    float64\n 23  Freq_24  208 non-null    float64\n 24  Freq_25  208 non-null    float64\n 25  Freq_26  208 non-null    float64\n 26  Freq_27  208 non-null    float64\n 27  Freq_28  208 non-null    float64\n 28  Freq_29  208 non-null    float64\n 29  Freq_30  208 non-null    float64\n 30  Freq_31  208 non-null    float64\n 31  Freq_32  208 non-null    float64\n 32  Freq_33  208 non-null    float64\n 33  Freq_34  208 non-null    float64\n 34  Freq_35  208 non-null    float64\n 35  Freq_36  208 non-null    float64\n 36  Freq_37  208 non-null    float64\n 37  Freq_38  208 non-null    float64\n 38  Freq_39  208 non-null    float64\n 39  Freq_40  208 non-null    float64\n 40  Freq_41  208 non-null    float64\n 41  Freq_42  208 non-null    float64\n 42  Freq_43  208 non-null    float64\n 43  Freq_44  208 non-null    float64\n 44  Freq_45  208 non-null    float64\n 45  Freq_46  208 non-null    float64\n 46  Freq_47  208 non-null    float64\n 47  Freq_48  208 non-null    float64\n 48  Freq_49  208 non-null    float64\n 49  Freq_50  208 non-null    float64\n 50  Freq_51  208 non-null    float64\n 51  Freq_52  208 non-null    float64\n 52  Freq_53  208 non-null    float64\n 53  Freq_54  208 non-null    float64\n 54  Freq_55  208 non-null    float64\n 55  Freq_56  208 non-null    float64\n 56  Freq_57  208 non-null    float64\n 57  Freq_58  208 non-null    float64\n 58  Freq_59  208 non-null    float64\n 59  Freq_60  208 non-null    float64\n 60  Label    208 non-null    object \ndtypes: float64(60), object(1)\nmemory usage: 99.3+ KB\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           Freq_1      Freq_2      Freq_3      Freq_4      Freq_5      Freq_6  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \nstd      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \nmin      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \nmax      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n\n           Freq_7      Freq_8      Freq_9     Freq_10  ...     Freq_51  \\\ncount  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \nmean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \nstd      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \nmin      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \nmax      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n\n          Freq_52     Freq_53     Freq_54     Freq_55     Freq_56     Freq_57  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \nstd      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \nmin      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \nmax      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n\n          Freq_58     Freq_59     Freq_60  \ncount  208.000000  208.000000  208.000000  \nmean     0.007949    0.007941    0.006507  \nstd      0.006470    0.006181    0.005031  \nmin      0.000300    0.000100    0.000600  \n25%      0.003600    0.003675    0.003100  \n50%      0.005800    0.006400    0.005300  \n75%      0.010350    0.010325    0.008525  \nmax      0.044000    0.036400    0.043900  \n\n[8 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Freq_1</th>\n      <th>Freq_2</th>\n      <th>Freq_3</th>\n      <th>Freq_4</th>\n      <th>Freq_5</th>\n      <th>Freq_6</th>\n      <th>Freq_7</th>\n      <th>Freq_8</th>\n      <th>Freq_9</th>\n      <th>Freq_10</th>\n      <th>...</th>\n      <th>Freq_51</th>\n      <th>Freq_52</th>\n      <th>Freq_53</th>\n      <th>Freq_54</th>\n      <th>Freq_55</th>\n      <th>Freq_56</th>\n      <th>Freq_57</th>\n      <th>Freq_58</th>\n      <th>Freq_59</th>\n      <th>Freq_60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>...</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.029164</td>\n      <td>0.038437</td>\n      <td>0.043832</td>\n      <td>0.053892</td>\n      <td>0.075202</td>\n      <td>0.104570</td>\n      <td>0.121747</td>\n      <td>0.134799</td>\n      <td>0.178003</td>\n      <td>0.208259</td>\n      <td>...</td>\n      <td>0.016069</td>\n      <td>0.013420</td>\n      <td>0.010709</td>\n      <td>0.010941</td>\n      <td>0.009290</td>\n      <td>0.008222</td>\n      <td>0.007820</td>\n      <td>0.007949</td>\n      <td>0.007941</td>\n      <td>0.006507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.022991</td>\n      <td>0.032960</td>\n      <td>0.038428</td>\n      <td>0.046528</td>\n      <td>0.055552</td>\n      <td>0.059105</td>\n      <td>0.061788</td>\n      <td>0.085152</td>\n      <td>0.118387</td>\n      <td>0.134416</td>\n      <td>...</td>\n      <td>0.012008</td>\n      <td>0.009634</td>\n      <td>0.007060</td>\n      <td>0.007301</td>\n      <td>0.007088</td>\n      <td>0.005736</td>\n      <td>0.005785</td>\n      <td>0.006470</td>\n      <td>0.006181</td>\n      <td>0.005031</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001500</td>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.010200</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.007500</td>\n      <td>0.011300</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000300</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.013350</td>\n      <td>0.016450</td>\n      <td>0.018950</td>\n      <td>0.024375</td>\n      <td>0.038050</td>\n      <td>0.067025</td>\n      <td>0.080900</td>\n      <td>0.080425</td>\n      <td>0.097025</td>\n      <td>0.111275</td>\n      <td>...</td>\n      <td>0.008425</td>\n      <td>0.007275</td>\n      <td>0.005075</td>\n      <td>0.005375</td>\n      <td>0.004150</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003675</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.022800</td>\n      <td>0.030800</td>\n      <td>0.034300</td>\n      <td>0.044050</td>\n      <td>0.062500</td>\n      <td>0.092150</td>\n      <td>0.106950</td>\n      <td>0.112100</td>\n      <td>0.152250</td>\n      <td>0.182400</td>\n      <td>...</td>\n      <td>0.013900</td>\n      <td>0.011400</td>\n      <td>0.009550</td>\n      <td>0.009300</td>\n      <td>0.007500</td>\n      <td>0.006850</td>\n      <td>0.005950</td>\n      <td>0.005800</td>\n      <td>0.006400</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.035550</td>\n      <td>0.047950</td>\n      <td>0.057950</td>\n      <td>0.064500</td>\n      <td>0.100275</td>\n      <td>0.134125</td>\n      <td>0.154000</td>\n      <td>0.169600</td>\n      <td>0.233425</td>\n      <td>0.268700</td>\n      <td>...</td>\n      <td>0.020825</td>\n      <td>0.016725</td>\n      <td>0.014900</td>\n      <td>0.014500</td>\n      <td>0.012100</td>\n      <td>0.010575</td>\n      <td>0.010425</td>\n      <td>0.010350</td>\n      <td>0.010325</td>\n      <td>0.008525</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.137100</td>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>...</td>\n      <td>0.100400</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 60 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# 결측치 확인","metadata":{}},{"cell_type":"code","source":"for a, x in df_train.isna().sum().items():\n    if x != 0:\n        print(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T04:59:07.474465Z","iopub.execute_input":"2025-09-15T04:59:07.474792Z","iopub.status.idle":"2025-09-15T04:59:07.481202Z","shell.execute_reply.started":"2025-09-15T04:59:07.474767Z","shell.execute_reply":"2025-09-15T04:59:07.480341Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# 바위 vs 지뢰 확인","metadata":{}},{"cell_type":"code","source":"plot_df = df_train.Label.value_counts()\nplot_df.plot(kind=\"bar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:06:32.824396Z","iopub.execute_input":"2025-09-15T05:06:32.824678Z","iopub.status.idle":"2025-09-15T05:06:33.066945Z","shell.execute_reply.started":"2025-09-15T05:06:32.824655Z","shell.execute_reply":"2025-09-15T05:06:33.066092Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='Label'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# 데이터 셋 준비","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_ds_pd, valid_ds_pd = train_test_split(df_train, test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:10:06.593989Z","iopub.execute_input":"2025-09-15T05:10:06.594299Z","iopub.status.idle":"2025-09-15T05:10:06.599617Z","shell.execute_reply.started":"2025-09-15T05:10:06.594277Z","shell.execute_reply":"2025-09-15T05:10:06.598876Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label='Label')\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label='Label')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:06:39.365853Z","iopub.execute_input":"2025-09-15T05:06:39.366609Z","iopub.status.idle":"2025-09-15T05:06:40.301635Z","shell.execute_reply.started":"2025-09-15T05:06:39.366585Z","shell.execute_reply":"2025-09-15T05:06:40.300641Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1757912800.116364      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 모델 훈련","metadata":{}},{"cell_type":"code","source":"import tensorflow_decision_forests as tfdf\n\nmodel = tfdf.keras.RandomForestModel()\nmodel.fit(x=train_ds,  validation_data=valid_ds)\n# validation_data 가 없으면 훈련만 진행, 있으면 검증까지 함","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:16:53.188487Z","iopub.execute_input":"2025-09-15T05:16:53.189099Z","iopub.status.idle":"2025-09-15T05:16:54.645879Z","shell.execute_reply.started":"2025-09-15T05:16:53.189075Z","shell.execute_reply":"2025-09-15T05:16:54.645158Z"}},"outputs":[{"name":"stdout","text":"Use /tmp/tmpjvh51jfk as temporary training directory\nReading training dataset...\nTraining dataset read in 0:00:00.775608. Found 164 examples.\nTraining model...\nModel trained in 0:00:00.095750\nCompiling model...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1757913413.986704      36 kernel.cc:782] Start Yggdrasil model training\nI0000 00:00:1757913413.986746      36 kernel.cc:783] Collect training examples\nI0000 00:00:1757913413.986754      36 kernel.cc:795] Dataspec guide:\ncolumn_guides {\n  column_name_pattern: \"^__LABEL$\"\n  type: CATEGORICAL\n  categorial {\n    min_vocab_frequency: 0\n    max_vocab_count: -1\n  }\n}\ndefault_column_guide {\n  categorial {\n    max_vocab_count: 2000\n  }\n  discretized_numerical {\n    maximum_num_bins: 255\n  }\n}\nignore_columns_without_guides: false\ndetect_numerical_as_discretized_numerical: false\n\nI0000 00:00:1757913413.987047      36 kernel.cc:401] Number of batches: 1\nI0000 00:00:1757913413.987069      36 kernel.cc:402] Number of examples: 164\nI0000 00:00:1757913413.987243      36 kernel.cc:802] Training dataset:\nNumber of records: 164\nNumber of columns: 61\n\nNumber of columns by type:\n\tNUMERICAL: 60 (98.3607%)\n\tCATEGORICAL: 1 (1.63934%)\n\nColumns:\n\nNUMERICAL: 60 (98.3607%)\n\t0: \"Freq_1\" NUMERICAL mean:0.0292744 min:0.0025 max:0.1371 sd:0.0236522\n\t1: \"Freq_10\" NUMERICAL mean:0.205749 min:0.0113 max:0.7106 sd:0.126822\n\t2: \"Freq_11\" NUMERICAL mean:0.231487 min:0.0289 max:0.7342 sd:0.126305\n\t3: \"Freq_12\" NUMERICAL mean:0.242456 min:0.0236 max:0.6552 sd:0.134397\n\t4: \"Freq_13\" NUMERICAL mean:0.273638 min:0.0184 max:0.7131 sd:0.135139\n\t5: \"Freq_14\" NUMERICAL mean:0.299057 min:0.0336 max:0.997 sd:0.162202\n\t6: \"Freq_15\" NUMERICAL mean:0.320364 min:0.0031 max:1 sd:0.204448\n\t7: \"Freq_16\" NUMERICAL mean:0.376424 min:0.0162 max:0.9988 sd:0.233491\n\t8: \"Freq_17\" NUMERICAL mean:0.414679 min:0.0349 max:1 sd:0.263681\n\t9: \"Freq_18\" NUMERICAL mean:0.448059 min:0.0375 max:1 sd:0.259053\n\t10: \"Freq_19\" NUMERICAL mean:0.498596 min:0.0494 max:1 sd:0.253356\n\t11: \"Freq_2\" NUMERICAL mean:0.0387713 min:0.0006 max:0.2339 sd:0.0338827\n\t12: \"Freq_20\" NUMERICAL mean:0.556932 min:0.0656 max:1 sd:0.253961\n\t13: \"Freq_21\" NUMERICAL mean:0.600852 min:0.0512 max:1 sd:0.250387\n\t14: \"Freq_22\" NUMERICAL mean:0.613655 min:0.0689 max:1 sd:0.255852\n\t15: \"Freq_23\" NUMERICAL mean:0.633493 min:0.0563 max:1 sd:0.252311\n\t16: \"Freq_24\" NUMERICAL mean:0.668295 min:0.0239 max:1 sd:0.243571\n\t17: \"Freq_25\" NUMERICAL mean:0.674207 min:0.0395 max:1 sd:0.24398\n\t18: \"Freq_26\" NUMERICAL mean:0.697955 min:0.1543 max:1 sd:0.234666\n\t19: \"Freq_27\" NUMERICAL mean:0.704666 min:0.0874 max:1 sd:0.249019\n\t20: \"Freq_28\" NUMERICAL mean:0.700905 min:0.0284 max:1 sd:0.233493\n\t21: \"Freq_29\" NUMERICAL mean:0.651719 min:0.0144 max:1 sd:0.242572\n\t22: \"Freq_3\" NUMERICAL mean:0.0446726 min:0.0015 max:0.3059 sd:0.0393052\n\t23: \"Freq_30\" NUMERICAL mean:0.590419 min:0.0613 max:1 sd:0.215626\n\t24: \"Freq_31\" NUMERICAL mean:0.511838 min:0.0482 max:0.9657 sd:0.205429\n\t25: \"Freq_32\" NUMERICAL mean:0.449521 min:0.0404 max:0.9306 sd:0.208607\n\t26: \"Freq_33\" NUMERICAL mean:0.429795 min:0.0477 max:1 sd:0.203141\n\t27: \"Freq_34\" NUMERICAL mean:0.407719 min:0.0212 max:0.9647 sd:0.229868\n\t28: \"Freq_35\" NUMERICAL mean:0.398369 min:0.0244 max:1 sd:0.251477\n\t29: \"Freq_36\" NUMERICAL mean:0.387731 min:0.0271 max:1 sd:0.257176\n\t30: \"Freq_37\" NUMERICAL mean:0.365838 min:0.0351 max:0.9497 sd:0.234454\n\t31: \"Freq_38\" NUMERICAL mean:0.342301 min:0.0383 max:1 sd:0.206756\n\t32: \"Freq_39\" NUMERICAL mean:0.321309 min:0.0371 max:0.9857 sd:0.199494\n\t33: \"Freq_4\" NUMERICAL mean:0.0530573 min:0.0058 max:0.4264 sd:0.0476368\n\t34: \"Freq_40\" NUMERICAL mean:0.307974 min:0.0117 max:0.9297 sd:0.176896\n\t35: \"Freq_41\" NUMERICAL mean:0.290955 min:0.036 max:0.8995 sd:0.171687\n\t36: \"Freq_42\" NUMERICAL mean:0.282152 min:0.0056 max:0.8246 sd:0.174118\n\t37: \"Freq_43\" NUMERICAL mean:0.2425 min:0 max:0.7517 sd:0.135\n\t38: \"Freq_44\" NUMERICAL mean:0.207824 min:0 max:0.5772 sd:0.124528\n\t39: \"Freq_45\" NUMERICAL mean:0.196445 min:0 max:0.6448 sd:0.149943\n\t40: \"Freq_46\" NUMERICAL mean:0.163507 min:0 max:0.7292 sd:0.135717\n\t41: \"Freq_47\" NUMERICAL mean:0.123484 min:0 max:0.5522 sd:0.0880652\n\t42: \"Freq_48\" NUMERICAL mean:0.0928116 min:0 max:0.3339 sd:0.0626363\n\t43: \"Freq_49\" NUMERICAL mean:0.0522055 min:0 max:0.1981 sd:0.0365607\n\t44: \"Freq_5\" NUMERICAL mean:0.0733073 min:0.0067 max:0.401 sd:0.0551481\n\t45: \"Freq_50\" NUMERICAL mean:0.0207165 min:0 max:0.0825 sd:0.0143356\n\t46: \"Freq_51\" NUMERICAL mean:0.0165701 min:0 max:0.1004 sd:0.0126126\n\t47: \"Freq_52\" NUMERICAL mean:0.0132799 min:0.0008 max:0.0709 sd:0.00959907\n\t48: \"Freq_53\" NUMERICAL mean:0.0105006 min:0.0005 max:0.039 sd:0.00740617\n\t49: \"Freq_54\" NUMERICAL mean:0.0109439 min:0.001 max:0.0352 sd:0.00750507\n\t50: \"Freq_55\" NUMERICAL mean:0.00972073 min:0.0006 max:0.0447 sd:0.00747362\n\t51: \"Freq_56\" NUMERICAL mean:0.00830427 min:0.0004 max:0.0394 sd:0.00578486\n\t52: \"Freq_57\" NUMERICAL mean:0.00796159 min:0.0007 max:0.0355 sd:0.00571863\n\t53: \"Freq_58\" NUMERICAL mean:0.00806159 min:0.0003 max:0.044 sd:0.00676139\n\t54: \"Freq_59\" NUMERICAL mean:0.00813171 min:0.0001 max:0.0364 sd:0.00636776\n\t55: \"Freq_6\" NUMERICAL mean:0.10089 min:0.0102 max:0.3823 sd:0.0544983\n\t56: \"Freq_60\" NUMERICAL mean:0.00657622 min:0.0006 max:0.0439 sd:0.00525496\n\t57: \"Freq_7\" NUMERICAL mean:0.119688 min:0.013 max:0.3729 sd:0.0600796\n\t58: \"Freq_8\" NUMERICAL mean:0.13131 min:0.0055 max:0.459 sd:0.0822346\n\t59: \"Freq_9\" NUMERICAL mean:0.176105 min:0.0075 max:0.6587 sd:0.109542\n\nCATEGORICAL: 1 (1.63934%)\n\t60: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\nI0000 00:00:1757913413.987334      36 kernel.cc:818] Configure learner\nI0000 00:00:1757913413.987586      36 kernel.cc:831] Training config:\nlearner: \"RANDOM_FOREST\"\nfeatures: \"^Freq_1$\"\nfeatures: \"^Freq_10$\"\nfeatures: \"^Freq_11$\"\nfeatures: \"^Freq_12$\"\nfeatures: \"^Freq_13$\"\nfeatures: \"^Freq_14$\"\nfeatures: \"^Freq_15$\"\nfeatures: \"^Freq_16$\"\nfeatures: \"^Freq_17$\"\nfeatures: \"^Freq_18$\"\nfeatures: \"^Freq_19$\"\nfeatures: \"^Freq_2$\"\nfeatures: \"^Freq_20$\"\nfeatures: \"^Freq_21$\"\nfeatures: \"^Freq_22$\"\nfeatures: \"^Freq_23$\"\nfeatures: \"^Freq_24$\"\nfeatures: \"^Freq_25$\"\nfeatures: \"^Freq_26$\"\nfeatures: \"^Freq_27$\"\nfeatures: \"^Freq_28$\"\nfeatures: \"^Freq_29$\"\nfeatures: \"^Freq_3$\"\nfeatures: \"^Freq_30$\"\nfeatures: \"^Freq_31$\"\nfeatures: \"^Freq_32$\"\nfeatures: \"^Freq_33$\"\nfeatures: \"^Freq_34$\"\nfeatures: \"^Freq_35$\"\nfeatures: \"^Freq_36$\"\nfeatures: \"^Freq_37$\"\nfeatures: \"^Freq_38$\"\nfeatures: \"^Freq_39$\"\nfeatures: \"^Freq_4$\"\nfeatures: \"^Freq_40$\"\nfeatures: \"^Freq_41$\"\nfeatures: \"^Freq_42$\"\nfeatures: \"^Freq_43$\"\nfeatures: \"^Freq_44$\"\nfeatures: \"^Freq_45$\"\nfeatures: \"^Freq_46$\"\nfeatures: \"^Freq_47$\"\nfeatures: \"^Freq_48$\"\nfeatures: \"^Freq_49$\"\nfeatures: \"^Freq_5$\"\nfeatures: \"^Freq_50$\"\nfeatures: \"^Freq_51$\"\nfeatures: \"^Freq_52$\"\nfeatures: \"^Freq_53$\"\nfeatures: \"^Freq_54$\"\nfeatures: \"^Freq_55$\"\nfeatures: \"^Freq_56$\"\nfeatures: \"^Freq_57$\"\nfeatures: \"^Freq_58$\"\nfeatures: \"^Freq_59$\"\nfeatures: \"^Freq_6$\"\nfeatures: \"^Freq_60$\"\nfeatures: \"^Freq_7$\"\nfeatures: \"^Freq_8$\"\nfeatures: \"^Freq_9$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 123456\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 16\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: 0\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  winner_take_all_inference: true\n  compute_oob_performances: true\n  compute_oob_variable_importances: false\n  num_oob_variable_importances_permutations: 1\n  bootstrap_training_dataset: true\n  bootstrap_size_ratio: 1\n  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n  sampling_with_replacement: true\n}\n\nI0000 00:00:1757913413.987705      36 kernel.cc:834] Deployment config:\ncache_path: \"/tmp/tmpjvh51jfk/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\nI0000 00:00:1757913413.987888     190 kernel.cc:895] Train model\nI0000 00:00:1757913413.988629     190 random_forest.cc:427] Training random forest on 164 example(s) and 60 feature(s).\nI0000 00:00:1757913413.990295     195 random_forest.cc:811] Training of tree  1/300 (tree index:1) done accuracy:0.766667 logloss:8.41019\nI0000 00:00:1757913413.991624     198 random_forest.cc:811] Training of tree  11/300 (tree index:10) done accuracy:0.737805 logloss:2.58646\nI0000 00:00:1757913413.993029     195 random_forest.cc:811] Training of tree  21/300 (tree index:20) done accuracy:0.798781 logloss:0.449856\nI0000 00:00:1757913413.994618     197 random_forest.cc:811] Training of tree  31/300 (tree index:29) done accuracy:0.780488 logloss:0.447941\nI0000 00:00:1757913413.996010     198 random_forest.cc:811] Training of tree  41/300 (tree index:39) done accuracy:0.792683 logloss:0.432224\nI0000 00:00:1757913413.997517     195 random_forest.cc:811] Training of tree  51/300 (tree index:50) done accuracy:0.792683 logloss:0.421998\nI0000 00:00:1757913413.998999     196 random_forest.cc:811] Training of tree  61/300 (tree index:61) done accuracy:0.804878 logloss:0.42831\nI0000 00:00:1757913414.000498     195 random_forest.cc:811] Training of tree  71/300 (tree index:71) done accuracy:0.798781 logloss:0.423776\nI0000 00:00:1757913414.001800     196 random_forest.cc:811] Training of tree  81/300 (tree index:80) done accuracy:0.798781 logloss:0.420931\nI0000 00:00:1757913414.003400     198 random_forest.cc:811] Training of tree  91/300 (tree index:90) done accuracy:0.810976 logloss:0.419092\nI0000 00:00:1757913414.004759     197 random_forest.cc:811] Training of tree  101/300 (tree index:100) done accuracy:0.817073 logloss:0.414427\nI0000 00:00:1757913414.006279     198 random_forest.cc:811] Training of tree  111/300 (tree index:110) done accuracy:0.823171 logloss:0.412236\nI0000 00:00:1757913414.007764     195 random_forest.cc:811] Training of tree  121/300 (tree index:120) done accuracy:0.817073 logloss:0.414898\nI0000 00:00:1757913414.009285     197 random_forest.cc:811] Training of tree  131/300 (tree index:129) done accuracy:0.810976 logloss:0.416163\nI0000 00:00:1757913414.010736     195 random_forest.cc:811] Training of tree  141/300 (tree index:140) done accuracy:0.829268 logloss:0.411808\nI0000 00:00:1757913414.012200     196 random_forest.cc:811] Training of tree  151/300 (tree index:150) done accuracy:0.829268 logloss:0.410359\nI0000 00:00:1757913414.013747     197 random_forest.cc:811] Training of tree  161/300 (tree index:160) done accuracy:0.835366 logloss:0.406975\nI0000 00:00:1757913414.015128     196 random_forest.cc:811] Training of tree  171/300 (tree index:170) done accuracy:0.829268 logloss:0.409399\nI0000 00:00:1757913414.016759     198 random_forest.cc:811] Training of tree  181/300 (tree index:180) done accuracy:0.835366 logloss:0.408828\nI0000 00:00:1757913414.018225     195 random_forest.cc:811] Training of tree  191/300 (tree index:190) done accuracy:0.829268 logloss:0.40958\nI0000 00:00:1757913414.019825     196 random_forest.cc:811] Training of tree  201/300 (tree index:200) done accuracy:0.841463 logloss:0.408777\nI0000 00:00:1757913414.021213     197 random_forest.cc:811] Training of tree  211/300 (tree index:210) done accuracy:0.853659 logloss:0.408707\nI0000 00:00:1757913414.022737     195 random_forest.cc:811] Training of tree  221/300 (tree index:220) done accuracy:0.853659 logloss:0.409528\nI0000 00:00:1757913414.024423     197 random_forest.cc:811] Training of tree  231/300 (tree index:230) done accuracy:0.847561 logloss:0.410301\nI0000 00:00:1757913414.025694     198 random_forest.cc:811] Training of tree  241/300 (tree index:240) done accuracy:0.835366 logloss:0.410997\nI0000 00:00:1757913414.027372     195 random_forest.cc:811] Training of tree  251/300 (tree index:250) done accuracy:0.835366 logloss:0.412374\nI0000 00:00:1757913414.028736     196 random_forest.cc:811] Training of tree  261/300 (tree index:260) done accuracy:0.829268 logloss:0.412544\nI0000 00:00:1757913414.030296     198 random_forest.cc:811] Training of tree  271/300 (tree index:270) done accuracy:0.835366 logloss:0.411322\nI0000 00:00:1757913414.031639     197 random_forest.cc:811] Training of tree  281/300 (tree index:280) done accuracy:0.829268 logloss:0.409727\nI0000 00:00:1757913414.033268     196 random_forest.cc:811] Training of tree  291/300 (tree index:290) done accuracy:0.835366 logloss:0.4098\nI0000 00:00:1757913414.034546     196 random_forest.cc:811] Training of tree  300/300 (tree index:299) done accuracy:0.810976 logloss:0.41056\nI0000 00:00:1757913414.034663     190 random_forest.cc:891] Final OOB metrics: accuracy:0.810976 logloss:0.41056\nI0000 00:00:1757913414.037253     190 kernel.cc:926] Export model in log directory: /tmp/tmpjvh51jfk with prefix 32c7ed70ddb84bf7\nI0000 00:00:1757913414.042172     190 kernel.cc:944] Save model in resources\nI0000 00:00:1757913414.043791      36 abstract_model.cc:914] Model self evaluation:\nNumber of predictions (without weights): 164\nNumber of predictions (with weights): 164\nTask: CLASSIFICATION\nLabel: __LABEL\n\nAccuracy: 0.810976  CI95[W][0.753535 0.859763]\nLogLoss: : 0.41056\nErrorRate: : 0.189024\n\nDefault Accuracy: : 0.518293\nDefault LogLoss: : 0.692478\nDefault ErrorRate: : 0.481707\n\nConfusion Table:\ntruth\\prediction\n    1   2\n1  73  12\n2  19  60\nTotal: 164\n\n\nI0000 00:00:1757913414.077580      36 decision_forest.cc:761] Model loaded with 300 root(s), 7538 node(s), and 60 input feature(s).\nI0000 00:00:1757913414.077620      36 abstract_model.cc:1404] Engine \"RandomForestOptPred\" built\n","output_type":"stream"},{"name":"stdout","text":"Model compiled.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<tf_keras.src.callbacks.History at 0x7f5444466190>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# 모델 검증","metadata":{}},{"cell_type":"code","source":"train_evaluation = model.evaluate(x=train_ds, return_dict=True)\nprint(\"train_data_evaluation:\")\nfor name, value in train_evaluation.items():\n    print(f\"- {name}: {value:.4f}\")\n\nvalid_evaluation = model.evaluate(x=valid_ds, return_dict=True)\nprint(\"valid_data_evaluation:\")\nfor name, value in train_evaluation.items():\n    print(f\"- {name}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T05:16:59.180688Z","iopub.execute_input":"2025-09-15T05:16:59.180956Z","iopub.status.idle":"2025-09-15T05:16:59.510851Z","shell.execute_reply.started":"2025-09-15T05:16:59.180937Z","shell.execute_reply":"2025-09-15T05:16:59.509990Z"}},"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 161ms/step - loss: 0.0000e+00\ntrain_data_evaluation:\n- loss: 0.0000\n1/1 [==============================] - 0s 152ms/step - loss: 0.0000e+00\nvalid_data_evaluation:\n- loss: 0.0000\n","output_type":"stream"}],"execution_count":15}]}